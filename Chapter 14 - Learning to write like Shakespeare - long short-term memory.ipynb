{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use definitions from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        self.id = np.random.randint(0, 100000) if id is None else id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for _, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "        \n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        \n",
    "        if grad_origin is not None:\n",
    "            if self.children[grad_origin.id] == 0:\n",
    "                return\n",
    "                raise Exception(\"cannot backprop more than once\")\n",
    "\n",
    "            self.children[grad_origin.id] -= 1\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        \n",
    "        assert grad.autograd == False\n",
    "        \n",
    "        if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):\n",
    "            if self.creation_op == \"neg\":\n",
    "                self.creators[0].backward(self.grad.__neg__(), self)\n",
    "            elif self.creation_op == \"add\":\n",
    "                self.creators[0].backward(self.grad, self)\n",
    "                self.creators[1].backward(self.grad, self)\n",
    "            elif self.creation_op == \"sub\":\n",
    "                positive_grad = Tensor(self.grad.data)\n",
    "                self.creators[0].backward(positive_grad, self)\n",
    "                negative_grad = Tensor(self.grad.__neg__().data)\n",
    "                self.creators[1].backward(negative_grad, self)\n",
    "            elif self.creation_op == \"mul\":\n",
    "                mul_grad_0 = self.grad * self.creators[0]\n",
    "                self.creators[0].backward(mul_grad_0, self)\n",
    "                mul_grad_1 = self.grad * self.creators[1]\n",
    "                self.creators[1].backward(mul_grad_1, self)\n",
    "            elif self.creation_op == \"transpose\":\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "            elif self.creation_op == \"mm\":\n",
    "                activations = self.creators[0]\n",
    "                weights = self.creators[1]\n",
    "                activations_grad = self.grad.mm(weights.transpose())\n",
    "                activations.backward(activations_grad)\n",
    "                weights_grad = self.grad.transpose().mm(activations).transpose()\n",
    "                weights.backward(weights_grad)\n",
    "            elif \"sum\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                ds = self.creators[0].data.shape[dim]\n",
    "                self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "            elif \"expand\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "            elif self.creation_op == \"sigmoid\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * self * (ones - self))\n",
    "            elif self.creation_op == \"tanh\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "            elif self.creation_op == \"index_select\":\n",
    "                new_grad = np.zeros_like(self.creators[0].data)\n",
    "                indices = self.index_select_indices.data.flatten()\n",
    "                grad_reshaped = grad.data.reshape(len(indices), -1)\n",
    "                for i in range(len(indices)):\n",
    "                    new_grad[indices[i]] += grad_reshaped[i]\n",
    "                self.creators[0].backward(Tensor(new_grad))\n",
    "            elif self.creation_op == \"cross_entropy\":\n",
    "                dx = self.softmax_output - self.target_dist\n",
    "                self.creators[0].backward(Tensor(dx))\n",
    "            \n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self, other], creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self, other], creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dimension):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dimension), autograd=True, creators=[self], creation_op=\"sum_\"+str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    def expand(self, dimension, copies):\n",
    "        transpose_cmd = list(range(0, len(self.data.shape)))\n",
    "        transpose_cmd.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape).transpose(transpose_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dimension))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd and x.autograd:\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self, x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            tensor = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op=\"index_select\")\n",
    "            tensor.index_select_indices = indices\n",
    "            return tensor\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        return softmax_output\n",
    "\n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * target_dist).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            tensor = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
    "            tensor.softmax_output = softmax_output\n",
    "            tensor.target_dist = target_dist\n",
    "            return tensor\n",
    "        return Tensor(loss)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        weights = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weights = Tensor(weights, autograd=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.biases = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weights)\n",
    "        if self.use_bias:\n",
    "            self.parameters.append(self.biases)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.use_bias:\n",
    "            return input.mm(self.weights) + self.biases.expand(0, len(input.data))\n",
    "        return input.mm(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        weights = (np.random.rand(vocab_size, dimensions) - 0.5) / dimensions\n",
    "        self.weights = Tensor(weights, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weights.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation=\"sigmoid\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "        \n",
    "        self.weights_ih = Linear(n_inputs, n_hidden)\n",
    "        self.weights_hh = Linear(n_hidden, n_hidden)\n",
    "        self.weights_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.weights_ih.get_parameters()\n",
    "        self.parameters += self.weights_hh.get_parameters()\n",
    "        self.parameters += self.weights_ho.get_parameters()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_previous_hidden = self.weights_hh.forward(hidden)\n",
    "        combined = self.weights_ih.forward(input) + from_previous_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.weights_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.data -= parameter.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                parameter.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/shakespeare/shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab consists of characters, not words!\n",
    "vocab = list(set(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dimensions=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=1\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int(len(indices) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches - 1) / bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iter:0\tBatch:1/195\tLoss:93.41064503197084\n",
      "\r",
      "Iter:0\tBatch:2/195\tLoss:7.7272044771709e+75\n",
      "\r",
      "Iter:0\tBatch:3/195\tLoss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tBatch:4/195\tLoss:nan\n",
      "Iter:0\tBatch:5/195\tLoss:nan\n",
      "Iter:0\tBatch:6/195\tLoss:nan\n",
      "Iter:0\tBatch:7/195\tLoss:nan\n",
      "Iter:0\tBatch:8/195\tLoss:nan\n",
      "Iter:0\tBatch:9/195\tLoss:nan\n",
      "Iter:0\tBatch:10/195\tLoss:nan\n",
      "Iter:0\tBatch:11/195\tLoss:nan\n",
      "Iter:0\tBatch:12/195\tLoss:nan\n",
      "Iter:0\tBatch:13/195\tLoss:nan\n",
      "Iter:0\tBatch:14/195\tLoss:nan\n",
      "Iter:0\tBatch:15/195\tLoss:nan\n",
      "Iter:0\tBatch:16/195\tLoss:nan\n",
      "Iter:0\tBatch:17/195\tLoss:nan\n",
      "Iter:0\tBatch:18/195\tLoss:nan\n",
      "Iter:0\tBatch:19/195\tLoss:nan\n",
      "Iter:0\tBatch:20/195\tLoss:nan\n",
      "Iter:0\tBatch:21/195\tLoss:nan\n",
      "Iter:0\tBatch:22/195\tLoss:nan\n",
      "Iter:0\tBatch:23/195\tLoss:nan\n",
      "Iter:0\tBatch:24/195\tLoss:nan\n",
      "Iter:0\tBatch:25/195\tLoss:nan\n",
      "Iter:0\tBatch:26/195\tLoss:nan\n",
      "Iter:0\tBatch:27/195\tLoss:nan\n",
      "Iter:0\tBatch:28/195\tLoss:nan\n",
      "Iter:0\tBatch:29/195\tLoss:nan\n",
      "Iter:0\tBatch:30/195\tLoss:nan\n",
      "Iter:0\tBatch:31/195\tLoss:nan\n",
      "Iter:0\tBatch:32/195\tLoss:nan\n",
      "Iter:0\tBatch:33/195\tLoss:nan\n",
      "Iter:0\tBatch:34/195\tLoss:nan\n",
      "Iter:0\tBatch:35/195\tLoss:nan\n",
      "Iter:0\tBatch:36/195\tLoss:nan\n",
      "Iter:0\tBatch:37/195\tLoss:nan\n",
      "Iter:0\tBatch:38/195\tLoss:nan\n",
      "Iter:0\tBatch:39/195\tLoss:nan\n",
      "Iter:0\tBatch:40/195\tLoss:nan\n",
      "Iter:0\tBatch:41/195\tLoss:nan\n",
      "Iter:0\tBatch:42/195\tLoss:nan\n",
      "Iter:0\tBatch:43/195\tLoss:nan\n",
      "Iter:0\tBatch:44/195\tLoss:nan\n",
      "Iter:0\tBatch:45/195\tLoss:nan\n",
      "Iter:0\tBatch:46/195\tLoss:nan\n",
      "Iter:0\tBatch:47/195\tLoss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tBatch:48/195\tLoss:nan\n",
      "Iter:0\tBatch:49/195\tLoss:nan\n",
      "Iter:0\tBatch:50/195\tLoss:nan\n",
      "Iter:0\tBatch:51/195\tLoss:nan\n",
      "Iter:0\tBatch:52/195\tLoss:nan\n",
      "Iter:0\tBatch:53/195\tLoss:nan\n",
      "Iter:0\tBatch:54/195\tLoss:nan\n",
      "Iter:0\tBatch:55/195\tLoss:nan\n",
      "Iter:0\tBatch:56/195\tLoss:nan\n",
      "Iter:0\tBatch:57/195\tLoss:nan\n",
      "Iter:0\tBatch:58/195\tLoss:nan\n",
      "Iter:0\tBatch:59/195\tLoss:nan\n",
      "Iter:0\tBatch:60/195\tLoss:nan\n",
      "Iter:0\tBatch:61/195\tLoss:nan\n",
      "Iter:0\tBatch:62/195\tLoss:nan\n",
      "Iter:0\tBatch:63/195\tLoss:nan\n",
      "Iter:0\tBatch:64/195\tLoss:nan\n",
      "Iter:0\tBatch:65/195\tLoss:nan\n",
      "Iter:0\tBatch:66/195\tLoss:nan\n",
      "Iter:0\tBatch:67/195\tLoss:nan\n",
      "Iter:0\tBatch:68/195\tLoss:nan\n",
      "Iter:0\tBatch:69/195\tLoss:nan\n",
      "Iter:0\tBatch:70/195\tLoss:nan\n",
      "Iter:0\tBatch:71/195\tLoss:nan\n",
      "Iter:0\tBatch:72/195\tLoss:nan\n",
      "Iter:0\tBatch:73/195\tLoss:nan\n",
      "Iter:0\tBatch:74/195\tLoss:nan\n",
      "Iter:0\tBatch:75/195\tLoss:nan\n",
      "Iter:0\tBatch:76/195\tLoss:nan\n",
      "Iter:0\tBatch:77/195\tLoss:nan\n",
      "Iter:0\tBatch:78/195\tLoss:nan\n",
      "Iter:0\tBatch:79/195\tLoss:nan\n",
      "Iter:0\tBatch:80/195\tLoss:nan\n",
      "Iter:0\tBatch:81/195\tLoss:nan\n",
      "Iter:0\tBatch:82/195\tLoss:nan\n",
      "Iter:0\tBatch:83/195\tLoss:nan\n",
      "Iter:0\tBatch:84/195\tLoss:nan\n",
      "Iter:0\tBatch:85/195\tLoss:nan\n",
      "Iter:0\tBatch:86/195\tLoss:nan\n",
      "Iter:0\tBatch:87/195\tLoss:nan\n",
      "Iter:0\tBatch:88/195\tLoss:nan\n",
      "Iter:0\tBatch:89/195\tLoss:nan\n",
      "Iter:0\tBatch:90/195\tLoss:nan\n",
      "Iter:0\tBatch:91/195\tLoss:nan\n",
      "Iter:0\tBatch:92/195\tLoss:nan\n",
      "Iter:0\tBatch:93/195\tLoss:nan\n",
      "Iter:0\tBatch:94/195\tLoss:nan\n",
      "Iter:0\tBatch:95/195\tLoss:nan\n",
      "Iter:0\tBatch:96/195\tLoss:nan\n",
      "Iter:0\tBatch:97/195\tLoss:nan\n",
      "Iter:0\tBatch:98/195\tLoss:nan\n",
      "Iter:0\tBatch:99/195\tLoss:nan\n",
      "Iter:0\tBatch:100/195\tLoss:nan\n",
      "Iter:0\tBatch:101/195\tLoss:nan\n",
      "Iter:0\tBatch:102/195\tLoss:nan\n",
      "Iter:0\tBatch:103/195\tLoss:nan\n",
      "Iter:0\tBatch:104/195\tLoss:nan\n",
      "Iter:0\tBatch:105/195\tLoss:nan\n",
      "Iter:0\tBatch:106/195\tLoss:nan\n",
      "Iter:0\tBatch:107/195\tLoss:nan\n",
      "Iter:0\tBatch:108/195\tLoss:nan\n",
      "Iter:0\tBatch:109/195\tLoss:nan\n",
      "Iter:0\tBatch:110/195\tLoss:nan\n",
      "Iter:0\tBatch:111/195\tLoss:nan\n",
      "Iter:0\tBatch:112/195\tLoss:nan\n",
      "Iter:0\tBatch:113/195\tLoss:nan\n",
      "Iter:0\tBatch:114/195\tLoss:nan\n",
      "Iter:0\tBatch:115/195\tLoss:nan\n",
      "Iter:0\tBatch:116/195\tLoss:nan\n",
      "Iter:0\tBatch:117/195\tLoss:nan\n",
      "Iter:0\tBatch:118/195\tLoss:nan\n",
      "Iter:0\tBatch:119/195\tLoss:nan\n",
      "Iter:0\tBatch:120/195\tLoss:nan\n",
      "Iter:0\tBatch:121/195\tLoss:nan\n",
      "Iter:0\tBatch:122/195\tLoss:nan\n",
      "Iter:0\tBatch:123/195\tLoss:nan\n",
      "Iter:0\tBatch:124/195\tLoss:nan\n",
      "Iter:0\tBatch:125/195\tLoss:nan\n",
      "Iter:0\tBatch:126/195\tLoss:nan\n",
      "Iter:0\tBatch:127/195\tLoss:nan\n",
      "Iter:0\tBatch:128/195\tLoss:nan\n",
      "Iter:0\tBatch:129/195\tLoss:nan\n",
      "Iter:0\tBatch:130/195\tLoss:nan\n",
      "Iter:0\tBatch:131/195\tLoss:nan\n",
      "Iter:0\tBatch:132/195\tLoss:nan\n",
      "Iter:0\tBatch:133/195\tLoss:nan\n",
      "Iter:0\tBatch:134/195\tLoss:nan\n",
      "Iter:0\tBatch:135/195\tLoss:nan\n",
      "Iter:0\tBatch:136/195\tLoss:nan\n",
      "Iter:0\tBatch:137/195\tLoss:nan\n",
      "Iter:0\tBatch:138/195\tLoss:nan\n",
      "Iter:0\tBatch:139/195\tLoss:nan\n",
      "Iter:0\tBatch:140/195\tLoss:nan\n",
      "Iter:0\tBatch:141/195\tLoss:nan\n",
      "Iter:0\tBatch:142/195\tLoss:nan\n",
      "Iter:0\tBatch:143/195\tLoss:nan\n",
      "Iter:0\tBatch:144/195\tLoss:nan\n",
      "Iter:0\tBatch:145/195\tLoss:nan\n",
      "Iter:0\tBatch:146/195\tLoss:nan\n",
      "Iter:0\tBatch:147/195\tLoss:nan\n",
      "Iter:0\tBatch:148/195\tLoss:nan\n",
      "Iter:0\tBatch:149/195\tLoss:nan\n",
      "Iter:0\tBatch:150/195\tLoss:nan\n",
      "Iter:0\tBatch:151/195\tLoss:nan\n",
      "Iter:0\tBatch:152/195\tLoss:nan\n",
      "Iter:0\tBatch:153/195\tLoss:nan\n",
      "Iter:0\tBatch:154/195\tLoss:nan\n",
      "Iter:0\tBatch:155/195\tLoss:nan\n",
      "Iter:0\tBatch:156/195\tLoss:nan\n",
      "Iter:0\tBatch:157/195\tLoss:nan\n",
      "Iter:0\tBatch:158/195\tLoss:nan\n",
      "Iter:0\tBatch:159/195\tLoss:nan\n",
      "Iter:0\tBatch:160/195\tLoss:nan\n",
      "Iter:0\tBatch:161/195\tLoss:nan\n",
      "Iter:0\tBatch:162/195\tLoss:nan\n",
      "Iter:0\tBatch:163/195\tLoss:nan\n",
      "Iter:0\tBatch:164/195\tLoss:nan\n",
      "Iter:0\tBatch:165/195\tLoss:nan\n",
      "Iter:0\tBatch:166/195\tLoss:nan\n",
      "Iter:0\tBatch:167/195\tLoss:nan\n",
      "Iter:0\tBatch:168/195\tLoss:nan\n",
      "Iter:0\tBatch:169/195\tLoss:nan\n",
      "Iter:0\tBatch:170/195\tLoss:nan\n",
      "Iter:0\tBatch:171/195\tLoss:nan\n",
      "Iter:0\tBatch:172/195\tLoss:nan\n",
      "Iter:0\tBatch:173/195\tLoss:nan\n",
      "Iter:0\tBatch:174/195\tLoss:nan\n",
      "Iter:0\tBatch:175/195\tLoss:nan\n",
      "Iter:0\tBatch:176/195\tLoss:nan\n",
      "Iter:0\tBatch:177/195\tLoss:nan\n",
      "Iter:0\tBatch:178/195\tLoss:nan\n",
      "Iter:0\tBatch:179/195\tLoss:nan\n",
      "Iter:0\tBatch:180/195\tLoss:nan\n",
      "Iter:0\tBatch:181/195\tLoss:nan\n",
      "Iter:0\tBatch:182/195\tLoss:nan\n",
      "Iter:0\tBatch:183/195\tLoss:nan\n",
      "Iter:0\tBatch:184/195\tLoss:nan\n",
      "Iter:0\tBatch:185/195\tLoss:nan\n",
      "Iter:0\tBatch:186/195\tLoss:nan\n",
      "Iter:0\tBatch:187/195\tLoss:nan\n",
      "Iter:0\tBatch:188/195\tLoss:nan\n",
      "Iter:0\tBatch:189/195\tLoss:nan\n",
      "Iter:0\tBatch:190/195\tLoss:nan\n",
      "Iter:0\tBatch:191/195\tLoss:nan\n",
      "Iter:0\tBatch:192/195\tLoss:nan\n",
      "Iter:0\tBatch:193/195\tLoss:nan\n",
      "Iter:0\tBatch:194/195\tLoss:nan\n",
      "Iter:0\tBatch:195/195\tLoss:nan\n"
     ]
    }
   ],
   "source": [
    "def train(iterations=iterations):\n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        n_batches = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(n_batches):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            \n",
    "            for t in range(bptt):\n",
    "                embed_input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=embed_input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                loss = batch_loss if t == 0 else loss + batch_loss\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data / bptt\n",
    "            \n",
    "            log = \"\\rIter:\" + str(iteration)\n",
    "            log += \"\\tBatch:\" + str(batch_i + 1) + \"/\" + str(len(input_batches))\n",
    "            log += \"\\tLoss:\" + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            print(log)\n",
    "        \n",
    "        optimizer.alpha *= 0.99\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    embed_input = Tensor(np.array([word2index[init_char]]))\n",
    "\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(embed_input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden = hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        maximum = (temp_dist > np.random.rand()).argmax()\n",
    "        char = vocab[maximum]\n",
    "        s += char\n",
    "        embed_input = Tensor(np.array([maximum]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid activations\n",
      "[0.93940638 0.96852968]\n",
      "[0.9919462  0.99121735]\n",
      "[0.99301385 0.99302901]\n",
      "[0.9930713  0.99307098]\n",
      "[0.99307285 0.99307285]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "\n",
      "Sigmoid gradients\n",
      "[0.03439552 0.03439552]\n",
      "[0.00118305 0.00118305]\n",
      "[4.06916726e-05 4.06916726e-05]\n",
      "[1.39961115e-06 1.39961115e-06]\n",
      "[4.81403643e-08 4.81403637e-08]\n",
      "[1.65582672e-09 1.65582765e-09]\n",
      "[5.69682675e-11 5.69667160e-11]\n",
      "[1.97259346e-12 1.97517920e-12]\n",
      "[8.45387597e-14 8.02306381e-14]\n",
      "[1.45938177e-14 2.16938983e-14]\n",
      "\n",
      "Relu activations\n",
      "[4.8135251  4.72615519]\n",
      "[23.71814585 23.98025559]\n",
      "[119.63916823 118.852839  ]\n",
      "[595.05052421 597.40951192]\n",
      "[2984.68857188 2977.61160877]\n",
      "[14895.13500696 14916.36589628]\n",
      "[74560.59859209 74496.90592414]\n",
      "[372548.22228863 372739.30029248]\n",
      "[1863505.42345854 1862932.18944699]\n",
      "[9315234.18124649 9316953.88328115]\n",
      "\n",
      "Relu gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "relu = lambda x: (x > 0).astype(float) * x\n",
    "\n",
    "weights = np.array([[1,4], [4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "\n",
    "print(\"Sigmoid activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "    \n",
    "print(\"\\nSigmoid gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient).dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print(\"\\nRelu activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "    \n",
    "print(\"\\nRelu gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_output = n_output\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        \n",
    "        self.hf = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_inputs, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "\n",
    "        f = self.xf.forward(input) + self.hf.forward(prev_hidden)\n",
    "        i = self.xi.forward(input) + self.hi.forward(prev_hidden)\n",
    "        o = self.xo.forward(input) + self.ho.forward(prev_hidden)\n",
    "        g = self.xc.forward(input) + self.hc.forward(prev_hidden)\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        \n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        \n",
    "        return output, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dimensions=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.1)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int(len(indices) / batch_size)\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches-1) / bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tAlpha:0.1\tBatch:1/249\tMinLoss:62.000190181742774\tLoss:62.000190181742774\n",
      "Iter:0\tAlpha:0.1\tBatch:2/249\tMinLoss:62.00009509080026\tLoss:62.00009509080026\n",
      "Iter:0\tAlpha:0.1\tBatch:3/249\tMinLoss:62.00006339295789\tLoss:62.00006339295789\n",
      "Iter:0\tAlpha:0.1\tBatch:4/249\tMinLoss:62.00004754476132\tLoss:62.00004754476132\n",
      "Iter:0\tAlpha:0.1\tBatch:5/249\tMinLoss:62.00003803612378\tLoss:62.00003803612378\n",
      "Iter:0\tAlpha:0.1\tBatch:6/249\tMinLoss:62.00003169649651\tLoss:62.00003169649651\n",
      "Iter:0\tAlpha:0.1\tBatch:7/249\tMinLoss:62.00002716818238\tLoss:62.00002716818238\n",
      "Iter:0\tAlpha:0.1\tBatch:8/249\tMinLoss:62.00002377184923\tLoss:62.00002377184923\n",
      "Iter:0\tAlpha:0.1\tBatch:9/249\tMinLoss:62.0000211303204\tLoss:62.0000211303204\n",
      "Iter:0\tAlpha:0.1\tBatch:10/249\tMinLoss:62.000019017384325\tLoss:62.000019017384325\n",
      "Iter:0\tAlpha:0.1\tBatch:11/249\tMinLoss:62.00001728798913\tLoss:62.00001728798913\n",
      "Iter:0\tAlpha:0.1\tBatch:12/249\tMinLoss:62.00001584723424\tLoss:62.00001584723424\n",
      "Iter:0\tAlpha:0.1\tBatch:13/249\tMinLoss:62.00001462805866\tLoss:62.00001462805866\n",
      "Iter:0\tAlpha:0.1\tBatch:14/249\tMinLoss:62.00001358318641\tLoss:62.00001358318641\n",
      "Iter:0\tAlpha:0.1\tBatch:15/249\tMinLoss:62.00001267724775\tLoss:62.00001267724775\n",
      "Iter:0\tAlpha:0.1\tBatch:16/249\tMinLoss:62.00001188492349\tLoss:62.00001188492349\n",
      "Iter:0\tAlpha:0.1\tBatch:17/249\tMinLoss:62.00001118555438\tLoss:62.00001118555438\n",
      "Iter:0\tAlpha:0.1\tBatch:18/249\tMinLoss:62.00001056413233\tLoss:62.00001056413233\n",
      "Iter:0\tAlpha:0.1\tBatch:19/249\tMinLoss:62.00001000834253\tLoss:62.00001000834253\n",
      "Iter:0\tAlpha:0.1\tBatch:20/249\tMinLoss:62.000009508039746\tLoss:62.000009508039746\n",
      "Iter:0\tAlpha:0.1\tBatch:21/249\tMinLoss:62.000009055290626\tLoss:62.000009055290626\n",
      "Iter:0\tAlpha:0.1\tBatch:22/249\tMinLoss:62.00000864347942\tLoss:62.00000864347942\n",
      "Iter:0\tAlpha:0.1\tBatch:23/249\tMinLoss:62.00000826767745\tLoss:62.00000826767745\n",
      "Iter:0\tAlpha:0.1\tBatch:24/249\tMinLoss:62.00000792324119\tLoss:62.00000792324119\n",
      "Iter:0\tAlpha:0.1\tBatch:25/249\tMinLoss:62.00000760624459\tLoss:62.00000760624459\n",
      "Iter:0\tAlpha:0.1\tBatch:26/249\tMinLoss:62.00000731373233\tLoss:62.00000731373233\n",
      "Iter:0\tAlpha:0.1\tBatch:27/249\tMinLoss:62.00000704279036\tLoss:62.00000704279036\n",
      "Iter:0\tAlpha:0.1\tBatch:28/249\tMinLoss:62.00000679139289\tLoss:62.00000679139289\n",
      "Iter:0\tAlpha:0.1\tBatch:29/249\tMinLoss:62.00000655711539\tLoss:62.00000655711539\n",
      "Iter:0\tAlpha:0.1\tBatch:30/249\tMinLoss:62.000006338591206\tLoss:62.000006338591206\n",
      "Iter:0\tAlpha:0.1\tBatch:31/249\tMinLoss:62.00000613418269\tLoss:62.00000613418269\n",
      "Iter:0\tAlpha:0.1\tBatch:32/249\tMinLoss:62.00000594258129\tLoss:62.00000594258129\n",
      "Iter:0\tAlpha:0.1\tBatch:33/249\tMinLoss:62.00000576260256\tLoss:62.00000576260256\n",
      "Iter:0\tAlpha:0.1\tBatch:34/249\tMinLoss:62.00000559322854\tLoss:62.00000559322854\n",
      "Iter:0\tAlpha:0.1\tBatch:35/249\tMinLoss:62.00000543349435\tLoss:62.00000543349435\n",
      "Iter:0\tAlpha:0.1\tBatch:36/249\tMinLoss:62.000005282663395\tLoss:62.000005282663395\n",
      "Iter:0\tAlpha:0.1\tBatch:37/249\tMinLoss:62.00000513997267\tLoss:62.00000513997267\n",
      "Iter:0\tAlpha:0.1\tBatch:38/249\tMinLoss:62.00000500480203\tLoss:62.00000500480203\n",
      "Iter:0\tAlpha:0.1\tBatch:39/249\tMinLoss:62.0000048764489\tLoss:62.0000048764489\n",
      "Iter:0\tAlpha:0.1\tBatch:40/249\tMinLoss:62.00000475462794\tLoss:62.00000475462794\n",
      "Iter:0\tAlpha:0.1\tBatch:41/249\tMinLoss:62.000004638784176\tLoss:62.000004638784176\n",
      "Iter:0\tAlpha:0.1\tBatch:42/249\tMinLoss:62.000004528416945\tLoss:62.000004528416945\n",
      "Iter:0\tAlpha:0.1\tBatch:43/249\tMinLoss:62.000004423238686\tLoss:62.000004423238686\n",
      "Iter:0\tAlpha:0.1\tBatch:44/249\tMinLoss:62.000004322729396\tLoss:62.000004322729396\n",
      "Iter:0\tAlpha:0.1\tBatch:45/249\tMinLoss:62.00000422680405\tLoss:62.00000422680405\n",
      "Iter:0\tAlpha:0.1\tBatch:46/249\tMinLoss:62.00000413502499\tLoss:62.00000413502499\n",
      "Iter:0\tAlpha:0.1\tBatch:47/249\tMinLoss:62.000004047064486\tLoss:62.000004047064486\n",
      "Iter:0\tAlpha:0.1\tBatch:48/249\tMinLoss:62.00000396292416\tLoss:62.00000396292416\n",
      "Iter:0\tAlpha:0.1\tBatch:49/249\tMinLoss:62.0000038821959\tLoss:62.0000038821959\n",
      "Iter:0\tAlpha:0.1\tBatch:50/249\tMinLoss:62.00000380484809\tLoss:62.00000380484809\n",
      "Iter:0\tAlpha:0.1\tBatch:51/249\tMinLoss:62.00000373031129\tLoss:62.00000373031129\n",
      "Iter:0\tAlpha:0.1\tBatch:52/249\tMinLoss:62.00000365882488\tLoss:62.00000365882488\n",
      "Iter:0\tAlpha:0.1\tBatch:53/249\tMinLoss:62.00000359018263\tLoss:62.00000359018263\n",
      "Iter:0\tAlpha:0.1\tBatch:54/249\tMinLoss:62.000003523868834\tLoss:62.000003523868834\n",
      "Iter:0\tAlpha:0.1\tBatch:55/249\tMinLoss:62.00000346001064\tLoss:62.00000346001064\n",
      "Iter:0\tAlpha:0.1\tBatch:56/249\tMinLoss:62.00000339858206\tLoss:62.00000339858206\n",
      "Iter:0\tAlpha:0.1\tBatch:57/249\tMinLoss:62.00000333933227\tLoss:62.00000333933227\n",
      "Iter:0\tAlpha:0.1\tBatch:58/249\tMinLoss:62.000003281828704\tLoss:62.000003281828704\n",
      "Iter:0\tAlpha:0.1\tBatch:59/249\tMinLoss:62.0000032264928\tLoss:62.0000032264928\n",
      "Iter:0\tAlpha:0.1\tBatch:60/249\tMinLoss:62.0000031730506\tLoss:62.0000031730506\n",
      "Iter:0\tAlpha:0.1\tBatch:61/249\tMinLoss:62.0000031213597\tLoss:62.0000031213597\n",
      "Iter:0\tAlpha:0.1\tBatch:62/249\tMinLoss:62.000003071422185\tLoss:62.000003071422185\n",
      "Iter:0\tAlpha:0.1\tBatch:63/249\tMinLoss:62.000003023195994\tLoss:62.000003023195994\n",
      "Iter:0\tAlpha:0.1\tBatch:64/249\tMinLoss:62.00000297626823\tLoss:62.00000297626823\n",
      "Iter:0\tAlpha:0.1\tBatch:65/249\tMinLoss:62.00000293078068\tLoss:62.00000293078068\n",
      "Iter:0\tAlpha:0.1\tBatch:66/249\tMinLoss:62.0000028866381\tLoss:62.0000028866381\n",
      "Iter:0\tAlpha:0.1\tBatch:67/249\tMinLoss:62.00000284376239\tLoss:62.00000284376239\n",
      "Iter:0\tAlpha:0.1\tBatch:68/249\tMinLoss:62.00000280253219\tLoss:62.00000280253219\n",
      "Iter:0\tAlpha:0.1\tBatch:69/249\tMinLoss:62.000002762383176\tLoss:62.000002762383176\n",
      "Iter:0\tAlpha:0.1\tBatch:70/249\tMinLoss:62.00000272365401\tLoss:62.00000272365401\n",
      "Iter:0\tAlpha:0.1\tBatch:71/249\tMinLoss:62.00000268566297\tLoss:62.00000268566297\n",
      "Iter:0\tAlpha:0.1\tBatch:72/249\tMinLoss:62.00000264884277\tLoss:62.00000264884277\n",
      "Iter:0\tAlpha:0.1\tBatch:73/249\tMinLoss:62.000002613062996\tLoss:62.000002613062996\n",
      "Iter:0\tAlpha:0.1\tBatch:74/249\tMinLoss:62.00000257853121\tLoss:62.00000257853121\n",
      "Iter:0\tAlpha:0.1\tBatch:75/249\tMinLoss:62.000002544986174\tLoss:62.000002544986174\n",
      "Iter:0\tAlpha:0.1\tBatch:76/249\tMinLoss:62.000002512067354\tLoss:62.000002512067354\n",
      "Iter:0\tAlpha:0.1\tBatch:77/249\tMinLoss:62.00000248026194\tLoss:62.00000248026194\n",
      "Iter:0\tAlpha:0.1\tBatch:78/249\tMinLoss:62.000002449213646\tLoss:62.000002449213646\n",
      "Iter:0\tAlpha:0.1\tBatch:79/249\tMinLoss:62.000002419261456\tLoss:62.000002419261456\n",
      "Iter:0\tAlpha:0.1\tBatch:80/249\tMinLoss:62.00000239001446\tLoss:62.00000239001446\n",
      "Iter:0\tAlpha:0.1\tBatch:81/249\tMinLoss:62.00000236170365\tLoss:62.00000236170365\n",
      "Iter:0\tAlpha:0.1\tBatch:82/249\tMinLoss:62.00000233363278\tLoss:62.00000233363278\n",
      "Iter:0\tAlpha:0.1\tBatch:83/249\tMinLoss:62.00000230624071\tLoss:62.00000230624071\n",
      "Iter:0\tAlpha:0.1\tBatch:84/249\tMinLoss:62.00000227949987\tLoss:62.00000227949987\n",
      "Iter:0\tAlpha:0.1\tBatch:85/249\tMinLoss:62.00000225397265\tLoss:62.00000225397265\n",
      "Iter:0\tAlpha:0.1\tBatch:86/249\tMinLoss:62.00000222903371\tLoss:62.00000222903371\n",
      "Iter:0\tAlpha:0.1\tBatch:87/249\tMinLoss:62.00000220533913\tLoss:62.00000220533913\n",
      "Iter:0\tAlpha:0.1\tBatch:88/249\tMinLoss:62.00000218141998\tLoss:62.00000218141998\n",
      "Iter:0\tAlpha:0.1\tBatch:89/249\tMinLoss:62.000002158683394\tLoss:62.000002158683394\n",
      "Iter:0\tAlpha:0.1\tBatch:90/249\tMinLoss:62.000002136275945\tLoss:62.000002136275945\n",
      "Iter:0\tAlpha:0.1\tBatch:91/249\tMinLoss:62.00000211475304\tLoss:62.00000211475304\n",
      "Iter:0\tAlpha:0.1\tBatch:92/249\tMinLoss:62.000002094116546\tLoss:62.000002094116546\n",
      "Iter:0\tAlpha:0.1\tBatch:93/249\tMinLoss:62.0000020733002\tLoss:62.0000020733002\n",
      "Iter:0\tAlpha:0.1\tBatch:94/249\tMinLoss:62.00000205231651\tLoss:62.00000205231651\n",
      "Iter:0\tAlpha:0.1\tBatch:95/249\tMinLoss:62.00000203208327\tLoss:62.00000203208327\n",
      "Iter:0\tAlpha:0.1\tBatch:96/249\tMinLoss:62.00000201395195\tLoss:62.00000201395195\n",
      "Iter:0\tAlpha:0.1\tBatch:97/249\tMinLoss:62.000001994924354\tLoss:62.000001994924354\n",
      "Iter:0\tAlpha:0.1\tBatch:98/249\tMinLoss:62.000001977244686\tLoss:62.000001977244686\n",
      "Iter:0\tAlpha:0.1\tBatch:99/249\tMinLoss:62.000001961313565\tLoss:62.000001961313565\n",
      "Iter:0\tAlpha:0.1\tBatch:100/249\tMinLoss:62.00000194541647\tLoss:62.00000194541647\n",
      "Iter:0\tAlpha:0.1\tBatch:101/249\tMinLoss:62.000001932709694\tLoss:62.000001932709694\n",
      "Iter:0\tAlpha:0.1\tBatch:102/249\tMinLoss:62.00000192050199\tLoss:62.00000192050199\n",
      "Iter:0\tAlpha:0.1\tBatch:103/249\tMinLoss:62.000001910748345\tLoss:62.000001910748345\n",
      "Iter:0\tAlpha:0.1\tBatch:104/249\tMinLoss:62.000001896617746\tLoss:62.000001896617746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tAlpha:0.1\tBatch:105/249\tMinLoss:62.000001883845165\tLoss:62.000001883845165\n",
      "Iter:0\tAlpha:0.1\tBatch:106/249\tMinLoss:62.0000018692058\tLoss:62.0000018692058\n",
      "Iter:0\tAlpha:0.1\tBatch:107/249\tMinLoss:62.00000186273306\tLoss:62.00000186273306\n",
      "Iter:0\tAlpha:0.1\tBatch:108/249\tMinLoss:62.00000186035378\tLoss:62.00000186035378\n",
      "Iter:0\tAlpha:0.1\tBatch:109/249\tMinLoss:62.00000185306598\tLoss:62.00000185306598\n",
      "Iter:0\tAlpha:0.1\tBatch:110/249\tMinLoss:62.00000184421731\tLoss:62.00000184421731\n",
      "Iter:0\tAlpha:0.1\tBatch:111/249\tMinLoss:62.00000184421731\tLoss:62.00000184774793\n",
      "Iter:0\tAlpha:0.1\tBatch:112/249\tMinLoss:62.000001842789914\tLoss:62.000001842789914\n",
      "Iter:0\tAlpha:0.1\tBatch:113/249\tMinLoss:62.00000183736669\tLoss:62.00000183736669\n",
      "Iter:0\tAlpha:0.1\tBatch:114/249\tMinLoss:62.00000183736669\tLoss:62.00000186765049\n",
      "Iter:0\tAlpha:0.1\tBatch:115/249\tMinLoss:62.00000183736669\tLoss:62.000001922056036\n",
      "Iter:0\tAlpha:0.1\tBatch:116/249\tMinLoss:62.00000183736669\tLoss:62.00000198239852\n",
      "Iter:0\tAlpha:0.1\tBatch:117/249\tMinLoss:62.00000183736669\tLoss:62.000002057467874\n",
      "Iter:0\tAlpha:0.1\tBatch:118/249\tMinLoss:62.00000183736669\tLoss:62.000002154157265\n",
      "Iter:0\tAlpha:0.1\tBatch:119/249\tMinLoss:62.00000183736669\tLoss:62.000002293229244\n",
      "Iter:0\tAlpha:0.1\tBatch:120/249\tMinLoss:62.00000183736669\tLoss:62.000002464056145\n",
      "Iter:0\tAlpha:0.1\tBatch:121/249\tMinLoss:62.00000183736669\tLoss:62.00000271744719\n",
      "Iter:0\tAlpha:0.1\tBatch:122/249\tMinLoss:62.00000183736669\tLoss:62.00000319125082\n",
      "Iter:0\tAlpha:0.1\tBatch:123/249\tMinLoss:62.00000183736669\tLoss:62.00000366054936\n",
      "Iter:0\tAlpha:0.1\tBatch:124/249\tMinLoss:62.00000183736669\tLoss:62.00000428206016\n",
      "Iter:0\tAlpha:0.1\tBatch:125/249\tMinLoss:62.00000183736669\tLoss:62.00000513414415\n",
      "Iter:0\tAlpha:0.1\tBatch:126/249\tMinLoss:62.00000183736669\tLoss:62.00000590020184\n",
      "Iter:0\tAlpha:0.1\tBatch:127/249\tMinLoss:62.00000183736669\tLoss:62.00000675349237\n",
      "Iter:0\tAlpha:0.1\tBatch:128/249\tMinLoss:62.00000183736669\tLoss:62.00000850505094\n",
      "Iter:0\tAlpha:0.1\tBatch:129/249\tMinLoss:62.00000183736669\tLoss:62.000010032049786\n",
      "Iter:0\tAlpha:0.1\tBatch:130/249\tMinLoss:62.00000183736669\tLoss:62.00001127890116\n",
      "Iter:0\tAlpha:0.1\tBatch:131/249\tMinLoss:62.00000183736669\tLoss:62.0000141242431\n",
      "Iter:0\tAlpha:0.1\tBatch:132/249\tMinLoss:62.00000183736669\tLoss:62.00001858279752\n",
      "Iter:0\tAlpha:0.1\tBatch:133/249\tMinLoss:62.00000183736669\tLoss:62.00002154055689\n",
      "Iter:0\tAlpha:0.1\tBatch:134/249\tMinLoss:62.00000183736669\tLoss:62.000028581704605\n",
      "Iter:0\tAlpha:0.1\tBatch:135/249\tMinLoss:62.00000183736669\tLoss:62.000033002157274\n",
      "Iter:0\tAlpha:0.1\tBatch:136/249\tMinLoss:62.00000183736669\tLoss:62.00004106634631\n",
      "Iter:0\tAlpha:0.1\tBatch:137/249\tMinLoss:62.00000183736669\tLoss:62.000052292556596\n",
      "Iter:0\tAlpha:0.1\tBatch:138/249\tMinLoss:62.00000183736669\tLoss:62.00006321197567\n",
      "Iter:0\tAlpha:0.1\tBatch:139/249\tMinLoss:62.00000183736669\tLoss:62.000072591526795\n",
      "Iter:0\tAlpha:0.1\tBatch:140/249\tMinLoss:62.00000183736669\tLoss:62.00010025145968\n",
      "Iter:0\tAlpha:0.1\tBatch:141/249\tMinLoss:62.00000183736669\tLoss:62.0001121262421\n",
      "Iter:0\tAlpha:0.1\tBatch:142/249\tMinLoss:62.00000183736669\tLoss:62.00013317567062\n",
      "Iter:0\tAlpha:0.1\tBatch:143/249\tMinLoss:62.00000183736669\tLoss:62.00014346639133\n",
      "Iter:0\tAlpha:0.1\tBatch:144/249\tMinLoss:62.00000183736669\tLoss:62.00016125903886\n",
      "Iter:0\tAlpha:0.1\tBatch:145/249\tMinLoss:62.00000183736669\tLoss:62.00016572876759\n",
      "Iter:0\tAlpha:0.1\tBatch:146/249\tMinLoss:62.00000183736669\tLoss:62.00019325796384\n",
      "Iter:0\tAlpha:0.1\tBatch:147/249\tMinLoss:62.00000183736669\tLoss:62.00015845188071\n",
      "Iter:0\tAlpha:0.1\tBatch:148/249\tMinLoss:62.00000183736669\tLoss:62.00009073184942\n",
      "Iter:0\tAlpha:0.1\tBatch:149/249\tMinLoss:61.999944965720374\tLoss:61.999944965720374\n",
      "Iter:0\tAlpha:0.1\tBatch:150/249\tMinLoss:61.99980669867321\tLoss:61.99980669867321\n",
      "Iter:0\tAlpha:0.1\tBatch:151/249\tMinLoss:61.99970674650578\tLoss:61.99970674650578\n",
      "Iter:0\tAlpha:0.1\tBatch:152/249\tMinLoss:61.99954666763781\tLoss:61.99954666763781\n",
      "Iter:0\tAlpha:0.1\tBatch:153/249\tMinLoss:61.99944343398131\tLoss:61.99944343398131\n",
      "Iter:0\tAlpha:0.1\tBatch:154/249\tMinLoss:61.999318159586714\tLoss:61.999318159586714\n",
      "Iter:0\tAlpha:0.1\tBatch:155/249\tMinLoss:61.99918659915563\tLoss:61.99918659915563\n",
      "Iter:0\tAlpha:0.1\tBatch:156/249\tMinLoss:61.99906255550667\tLoss:61.99906255550667\n",
      "Iter:0\tAlpha:0.1\tBatch:157/249\tMinLoss:61.99894050396833\tLoss:61.99894050396833\n",
      "Iter:0\tAlpha:0.1\tBatch:158/249\tMinLoss:61.998825427544084\tLoss:61.998825427544084\n",
      "Iter:0\tAlpha:0.1\tBatch:159/249\tMinLoss:61.99864442474704\tLoss:61.99864442474704\n",
      "Iter:0\tAlpha:0.1\tBatch:160/249\tMinLoss:61.99847077496569\tLoss:61.99847077496569\n",
      "Iter:0\tAlpha:0.1\tBatch:161/249\tMinLoss:61.998182775152806\tLoss:61.998182775152806\n",
      "Iter:0\tAlpha:0.1\tBatch:162/249\tMinLoss:61.99784098962586\tLoss:61.99784098962586\n",
      "Iter:0\tAlpha:0.1\tBatch:163/249\tMinLoss:61.9975131603307\tLoss:61.9975131603307\n",
      "Iter:0\tAlpha:0.1\tBatch:164/249\tMinLoss:61.99696505634161\tLoss:61.99696505634161\n",
      "Iter:0\tAlpha:0.1\tBatch:165/249\tMinLoss:61.99510565059113\tLoss:61.99510565059113\n",
      "Iter:0\tAlpha:0.1\tBatch:166/249\tMinLoss:61.98963324208747\tLoss:61.98963324208747\n",
      "Iter:0\tAlpha:0.1\tBatch:167/249\tMinLoss:61.98689720485277\tLoss:61.98689720485277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: invalid value encountered in multiply\n",
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:112: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tAlpha:0.1\tBatch:168/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:169/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:170/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:171/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:172/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:173/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:174/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:175/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:176/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:177/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:178/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:179/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:180/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:181/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:182/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:183/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:184/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:185/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:186/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:187/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:188/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:189/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:190/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:191/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:192/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:193/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:194/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:195/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:196/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:197/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:198/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:199/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:200/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:201/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:202/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:203/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:204/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:205/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:206/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:207/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:208/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:209/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:210/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:211/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:212/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:213/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:214/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:215/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:216/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:217/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:218/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:219/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:220/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:221/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:222/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:223/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:224/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:225/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:226/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:227/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:228/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:229/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:230/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:231/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:232/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:233/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:234/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:235/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:236/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:237/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:238/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:239/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:240/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:241/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:242/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:243/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:244/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:245/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:246/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:247/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:248/249\tMinLoss:61.98689720485277\tLoss:nan\n",
      "Iter:0\tAlpha:0.1\tBatch:249/249\tMinLoss:61.98689720485277\tLoss:nan\n"
     ]
    }
   ],
   "source": [
    "def train(iterations=1):\n",
    "    min_loss = 1000\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                embed_input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=embed_input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                \n",
    "                if t == 0:\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            \n",
    "            loss = losses[-1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data / bptt\n",
    "            \n",
    "            epoch_loss = np.exp(total_loss/(batch_i + 1))\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "            \n",
    "            log = \"Iter:\" + str(iteration)\n",
    "            log += \"\\tAlpha:\" + str(optimizer.alpha)[0:5]\n",
    "            log += \"\\tBatch:\" + str(batch_i + 1) + \"/\" + str(batches_to_train)\n",
    "            log += \"\\tMinLoss:\" + str(min_loss)\n",
    "            log += \"\\tLoss:\" + str(epoch_loss)\n",
    "            if batch_i % 1 ==0:\n",
    "                print(log)\n",
    "        \n",
    "    optimizer.alpha *= 0.99\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
