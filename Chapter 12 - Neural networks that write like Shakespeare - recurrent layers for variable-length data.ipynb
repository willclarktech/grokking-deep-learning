{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(\"data/imdb/reviews.txt\")\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/imdb/labels.txt\")\n",
    "raw_labels = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sets = [set(filter(lambda token: len(token) > 0, review.split(\" \"))) for review in raw_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "for token_set in token_sets:\n",
    "    for word in token_set:\n",
    "        vocab_set.add(word)\n",
    "vocab = list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2index = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = [list(set([word2index[token] for token in token_set])) for token_set in token_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = [int(label == \"positive\\n\") for label in raw_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterations = 2\n",
    "hidden_size = 100\n",
    "weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:23999 Training accuracy:0.830542\n",
      "I:23999 Training accuracy:0.865479\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for iteration in range(iterations):\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "        x = input_dataset[i]\n",
    "        y = target_dataset[i]\n",
    "\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2))\n",
    "        \n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "        \n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "        \n",
    "        if np.abs(layer_2_delta) < 0.5:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    if iteration % 1 == 0:\n",
    "        accuracy = correct/total\n",
    "        print(\"I:%d Training accuracy:%f\" % (i, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = weights_0_1 * norms\n",
    "\n",
    "def make_sentence_vector(words):\n",
    "    indices = list(map(lambda x:word2index[x], filter(lambda x:x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews2vectors = list()\n",
    "for review in token_sets:\n",
    "    reviews2vectors.append(make_sentence_vector(review))\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sentence_vector(review)\n",
    "    \n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    \n",
    "    most_similar = list()\n",
    "    for idx, score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:400])\n",
    "    \n",
    "    return most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the characters are unlikeable and the script is awful . it  s a waste of the talents of deneuve and auteuil .  \\n',\n",
       " 'this movie is so bad  it can only be compared to the all  time worst  comedy   police academy  . no laughs throughout the movie . do something worthwhile  anything really . just don  t waste your time on this garbage .  \\n',\n",
       " 'this was one of the worst movies i have ever seen . the plot is awful  and the acting is worse . the jokes that are attempted absolutley suck . don  t bother to waste your time on a dumb movie such as this . and if for some reason that you do want to see this movie  don  t watch it with your parents .  \\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['boring', 'awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([0.1,0.2,0.3])\n",
    "c = np.array([-1,-0.5,0])\n",
    "d = np.array([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "this = np.array([2,4,6])\n",
    "movie = np.array([10,10,10])\n",
    "rocks = np.array([9,8,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 22 23]\n",
      "[21. 22. 23.]\n"
     ]
    }
   ],
   "source": [
    "print(this + movie + rocks)\n",
    "print((this.dot(identity) + movie).dot(identity) + rocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "word_vectors['yankees'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['bears'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['braves'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['red'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['sox'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['lose'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['defeat'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['beat'] = np.array([[0.,0.,0.]])\n",
    "word_vectors['tie'] = np.array([[0.,0.,0.]])\n",
    "\n",
    "sent2output = np.random.rand(3, len(word_vectors))\n",
    "identity = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "layer_0 = word_vectors['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vectors['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vectors['defeat']\n",
    "\n",
    "prediction = softmax(layer_2.dot(sent2output))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,0,0,0,0,0,0,0,0])\n",
    "prediction_delta = prediction - y\n",
    "\n",
    "layer_2_delta = prediction_delta.dot(sent2output.T)\n",
    "defeat_delta = layer_2_delta * 1\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta = layer_1_delta * 1\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "\n",
    "word_vectors['red'] -= layer_0_delta * alpha\n",
    "word_vectors['sox'] -= sox_delta * alpha\n",
    "word_vectors['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2, prediction_delta) * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 Mary moved to the bathroom.\\n',\n",
       " '2 John went to the hallway.\\n',\n",
       " '3 Where is Mary? \\tbathroom\\t1\\n',\n",
       " '4 Daniel went back to the hallway.\\n',\n",
       " '5 Sandra moved to the garden.\\n',\n",
       " '6 Where is Daniel? \\thallway\\t4\\n',\n",
       " '7 John moved to the office.\\n',\n",
       " '8 Sandra journeyed to the bathroom.\\n',\n",
       " '9 Where is Daniel? \\thallway\\t4\\n',\n",
       " '10 Mary moved to the hallway.\\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/babi/tasksv11/en/qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "raw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['mary', 'moved', 'to', 'the', 'bathroom'],\n",
       " ['john', 'went', 'to', 'the', 'hallway'],\n",
       " ['where', 'is', 'mary', 'bathroom', '1']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"?\", \"\").replace(\".\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \" \").replace(\"  \", \" \").split(\" \")[1:])\n",
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "embed_size = 10\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1\n",
    "recurrent = np.eye(embed_size)\n",
    "start = np.zeros(embed_size)\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
    "one_hot = np.eye(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    predictions = list()\n",
    "    for word in sent:\n",
    "        layer = {}\n",
    "        layer['prediction'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
    "        loss -= np.log(layer['prediction'][word])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[word]\n",
    "        layers.append(layer)\n",
    "    \n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 28.988171\n",
      "Perplexity: 28.898053\n",
      "Perplexity: 28.767948\n",
      "Perplexity: 28.547833\n",
      "Perplexity: 28.139522\n",
      "Perplexity: 27.307758\n",
      "Perplexity: 25.330364\n",
      "Perplexity: 20.199883\n",
      "Perplexity: 16.604822\n",
      "Perplexity: 14.577174\n",
      "Perplexity: 13.475768\n",
      "Perplexity: 12.831068\n",
      "Perplexity: 12.111374\n",
      "Perplexity: 11.103517\n",
      "Perplexity: 9.684804\n",
      "Perplexity: 7.841190\n",
      "Perplexity: 6.322451\n",
      "Perplexity: 5.512585\n",
      "Perplexity: 5.029377\n",
      "Perplexity: 4.687389\n",
      "Perplexity: 4.448744\n",
      "Perplexity: 4.285143\n",
      "Perplexity: 4.168195\n",
      "Perplexity: 4.079776\n",
      "Perplexity: 4.011216\n",
      "Perplexity: 3.957375\n",
      "Perplexity: 3.912866\n",
      "Perplexity: 3.873156\n",
      "Perplexity: 3.835707\n",
      "Perplexity: 3.799664\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(30000):\n",
    "    sent = words2indices(tokens[iteration % len(tokens)][1:])\n",
    "    layers, loss = predict(sent)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx - 1]\n",
    "\n",
    "        if layer_idx > 0:\n",
    "            layer['output_delta'] = layer['prediction'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.T)\n",
    "            \n",
    "            if layer_idx == len(layers) - 1:\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx + 1]['hidden_delta'].dot(recurrent.T)\n",
    "\n",
    "        else: # first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta'].dot(recurrent.T)\n",
    "    \n",
    "    start -= layers[0]['hidden_delta'] * alpha / len(sent)\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / len(sent)\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / len(sent)\n",
    "        \n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / len(sent)\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        print(\"Perplexity: %f\" % np.exp(loss/len(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra', 'moved', 'to', 'the', 'garden']\n",
      "Previous input: sandra\tTrue: moved\tPrediction: to\n",
      "Previous input: moved\tTrue: to\tPrediction: to\n",
      "Previous input: to\tTrue: the\tPrediction: the\n",
      "Previous input: the\tTrue: garden\tPrediction: bedroom\n"
     ]
    }
   ],
   "source": [
    "sent_index = 4\n",
    "layers, loss = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "for i, layer in enumerate(layers[1:-1]):\n",
    "    inp = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i+1]\n",
    "    prediction = vocab[layer['prediction'].argmax()]\n",
    "    print(\"Previous input: %s\\tTrue: %s\\tPrediction: %s\" % (inp, true, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
