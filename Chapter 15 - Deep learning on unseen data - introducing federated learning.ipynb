{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import copy\n",
    "import numpy as np\n",
    "import phe\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_lines = codecs.open('data/enron/spam.txt', 'r', encoding='utf-8', errors='ignore').readlines()\n",
    "ham_lines = codecs.open('data/enron/ham.txt', 'r', encoding='utf-8', errors='ignore').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown = '<unk>'\n",
    "vocab_set = set([unknown])\n",
    "spam = [set(row[:-2].split(\" \")) for row in spam_lines]\n",
    "ham = [set(row[:-2].split(\" \")) for row in ham_lines]\n",
    "\n",
    "for row in spam + ham:\n",
    "    for word in row:\n",
    "        vocab_set.add(word)\n",
    "\n",
    "vocab = list(vocab_set)\n",
    "word2index = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indices(inp, length=500):\n",
    "    indices = []\n",
    "    for line in inp:\n",
    "        if len(line) < length:\n",
    "            line = list(line) + [unknown] * (length - len(line))\n",
    "            idxs = [word2index[word] for word in line]\n",
    "            indices.append(idxs)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_idx = to_indices(spam)\n",
    "ham_idx = to_indices(ham)\n",
    "\n",
    "train_spam_idx = spam_idx[0:-1000]\n",
    "train_ham_idx = ham_idx[0:-1000]\n",
    "\n",
    "test_spam_idx = spam_idx[-1000:]\n",
    "test_ham_idx = ham_idx[-1000:]\n",
    "\n",
    "train_data = []\n",
    "train_target = []\n",
    "test_data = []\n",
    "test_target = []\n",
    "\n",
    "for i in range(max(len(train_spam_idx), len(train_ham_idx))):\n",
    "    train_data.append(train_spam_idx[i % len(train_spam_idx)])\n",
    "    train_target.append([1])\n",
    "    train_data.append(train_ham_idx[i % len(train_ham_idx)])\n",
    "    train_target.append([0])\n",
    "\n",
    "for i in range(max(len(test_spam_idx), len(test_ham_idx))):\n",
    "    test_data.append(test_spam_idx[i % len(test_spam_idx)])\n",
    "    test_target.append([1])\n",
    "    test_data.append(test_ham_idx[i % len(test_ham_idx)])\n",
    "    test_target.append([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        self.id = np.random.randint(0, 100000) if id is None else id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for _, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "        \n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        \n",
    "        if grad_origin is not None:\n",
    "            if self.children[grad_origin.id] == 0:\n",
    "                return\n",
    "                raise Exception(\"cannot backprop more than once\")\n",
    "\n",
    "            self.children[grad_origin.id] -= 1\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        \n",
    "        assert grad.autograd == False\n",
    "        \n",
    "        if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):\n",
    "            if self.creation_op == \"neg\":\n",
    "                self.creators[0].backward(self.grad.__neg__(), self)\n",
    "            elif self.creation_op == \"add\":\n",
    "                self.creators[0].backward(self.grad, self)\n",
    "                self.creators[1].backward(self.grad, self)\n",
    "            elif self.creation_op == \"sub\":\n",
    "                positive_grad = Tensor(self.grad.data)\n",
    "                self.creators[0].backward(positive_grad, self)\n",
    "                negative_grad = Tensor(self.grad.__neg__().data)\n",
    "                self.creators[1].backward(negative_grad, self)\n",
    "            elif self.creation_op == \"mul\":\n",
    "                mul_grad_0 = self.grad * self.creators[0]\n",
    "                self.creators[0].backward(mul_grad_0, self)\n",
    "                mul_grad_1 = self.grad * self.creators[1]\n",
    "                self.creators[1].backward(mul_grad_1, self)\n",
    "            elif self.creation_op == \"transpose\":\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "            elif self.creation_op == \"mm\":\n",
    "                activations = self.creators[0]\n",
    "                weights = self.creators[1]\n",
    "                activations_grad = self.grad.mm(weights.transpose())\n",
    "                activations.backward(activations_grad)\n",
    "                weights_grad = self.grad.transpose().mm(activations).transpose()\n",
    "                weights.backward(weights_grad)\n",
    "            elif \"sum\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                ds = self.creators[0].data.shape[dim]\n",
    "                self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "            elif \"expand\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "            elif self.creation_op == \"sigmoid\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * self * (ones - self))\n",
    "            elif self.creation_op == \"tanh\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "            elif self.creation_op == \"index_select\":\n",
    "                new_grad = np.zeros_like(self.creators[0].data)\n",
    "                indices = self.index_select_indices.data.flatten()\n",
    "                grad_reshaped = grad.data.reshape(len(indices), -1)\n",
    "                for i in range(len(indices)):\n",
    "                    new_grad[indices[i]] += grad_reshaped[i]\n",
    "                self.creators[0].backward(Tensor(new_grad))\n",
    "            elif self.creation_op == \"cross_entropy\":\n",
    "                dx = self.softmax_output - self.target_dist\n",
    "                self.creators[0].backward(Tensor(dx))\n",
    "            \n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self, other], creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self, other], creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dimension):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dimension), autograd=True, creators=[self], creation_op=\"sum_\"+str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    def expand(self, dimension, copies):\n",
    "        transpose_cmd = list(range(0, len(self.data.shape)))\n",
    "        transpose_cmd.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape).transpose(transpose_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dimension))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd and x.autograd:\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self, x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            tensor = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op=\"index_select\")\n",
    "            tensor.index_select_indices = indices\n",
    "            return tensor\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        return softmax_output\n",
    "\n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * target_dist).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            tensor = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
    "            tensor.softmax_output = softmax_output\n",
    "            tensor.target_dist = target_dist\n",
    "            return tensor\n",
    "        return Tensor(loss)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        weights = (np.random.rand(vocab_size, dimensions) - 0.5) / dimensions\n",
    "        self.weights = Tensor(weights, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weights.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        diff = prediction - target\n",
    "        return (diff * diff).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.data -= parameter.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                parameter.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
    "    n_batches = int(len(input_data) / batch_size)\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(parameters=model.get_parameters(), alpha=0.01)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        iteration_loss = 0\n",
    "        for b_i in range(n_batches):\n",
    "            model.weights.data[word2index[unknown]] *= 0\n",
    "            inp = Tensor(input_data[b_i*batch_size:(b_i + 1)*batch_size], autograd=True)\n",
    "            target = Tensor(target_data[b_i*batch_size:(b_i + 1)*batch_size], autograd=True)\n",
    "            \n",
    "            output = model.forward(inp)\n",
    "            prediction = output.sum(1).sigmoid()\n",
    "            loss = criterion.forward(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iteration_loss += loss.data[0] / batch_size\n",
    "            print('Loss: %f' % (iteration_loss / (b_i + 1)))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, input_data, target_data):\n",
    "    model.weights.data[word2index[unknown]] *= 0\n",
    "    \n",
    "    inp = Tensor(input_data, autograd=True)\n",
    "    target = Tensor(target_data, autograd=True)\n",
    "    \n",
    "    prediction = model.forward(inp).sum(1).sigmoid()\n",
    "    return ((prediction.data > 0.5) == target.data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dimensions=1)\n",
    "model.weights.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.250000\n",
      "Loss: 0.219082\n",
      "Loss: 0.293608\n",
      "Loss: 0.259571\n",
      "Loss: 0.223674\n",
      "Loss: 0.204721\n",
      "Loss: 0.182112\n",
      "Loss: 0.163808\n",
      "Loss: 0.150422\n",
      "Loss: 0.140804\n",
      "Loss: 0.132100\n",
      "Loss: 0.125227\n",
      "Loss: 0.118157\n",
      "Loss: 0.113585\n",
      "Loss: 0.110229\n",
      "Loss: 0.106477\n",
      "Loss: 0.102434\n",
      "Loss: 0.098753\n",
      "Loss: 0.094793\n",
      "Loss: 0.091434\n",
      "Loss: 0.088385\n",
      "Loss: 0.085427\n",
      "Loss: 0.082804\n",
      "Loss: 0.080167\n",
      "Loss: 0.078222\n",
      "Loss: 0.076305\n",
      "Loss: 0.074647\n",
      "Loss: 0.072889\n",
      "Loss: 0.072076\n",
      "Loss: 0.070552\n",
      "Loss: 0.069271\n",
      "Loss: 0.067928\n",
      "Loss: 0.066542\n",
      "Loss: 0.065171\n",
      "Loss: 0.063914\n",
      "Loss: 0.062610\n",
      "Loss: 0.061345\n",
      "Loss: 0.060248\n",
      "Loss: 0.059126\n",
      "Loss: 0.058162\n",
      "Loss: 0.057278\n",
      "Loss: 0.056440\n",
      "Loss: 0.055709\n",
      "Loss: 0.055264\n",
      "Loss: 0.054511\n",
      "Loss: 0.053940\n",
      "Loss: 0.053269\n",
      "Loss: 0.052553\n",
      "Loss: 0.051779\n",
      "Loss: 0.051058\n",
      "Loss: 0.050299\n",
      "Loss: 0.049647\n",
      "Loss: 0.048958\n",
      "Loss: 0.048369\n",
      "Loss: 0.047765\n",
      "Loss: 0.047195\n",
      "Loss: 0.046704\n",
      "Loss: 0.046376\n",
      "Loss: 0.045985\n",
      "Loss: 0.045565\n",
      "Loss: 0.045156\n",
      "Loss: 0.044695\n",
      "Loss: 0.044248\n",
      "Loss: 0.043722\n",
      "Loss: 0.043226\n",
      "Loss: 0.042773\n",
      "Loss: 0.042351\n",
      "Loss: 0.041914\n",
      "Loss: 0.041529\n",
      "Loss: 0.041072\n",
      "Loss: 0.040763\n",
      "Loss: 0.040440\n",
      "Loss: 0.040291\n",
      "Loss: 0.039999\n",
      "Loss: 0.039731\n",
      "Loss: 0.039411\n",
      "Loss: 0.039121\n",
      "Loss: 0.038786\n",
      "Loss: 0.038454\n",
      "Loss: 0.038112\n",
      "Loss: 0.037758\n",
      "Loss: 0.037450\n",
      "Loss: 0.037140\n",
      "% Correct on test set: 98.650000\n",
      "Loss: 0.014960\n",
      "Loss: 0.014631\n",
      "Loss: 0.015367\n",
      "Loss: 0.015364\n",
      "Loss: 0.014306\n",
      "Loss: 0.013921\n",
      "Loss: 0.013472\n",
      "Loss: 0.012907\n",
      "Loss: 0.012698\n",
      "Loss: 0.012484\n",
      "Loss: 0.012423\n",
      "Loss: 0.012399\n",
      "Loss: 0.012572\n",
      "Loss: 0.013157\n",
      "Loss: 0.013395\n",
      "Loss: 0.013551\n",
      "Loss: 0.013583\n",
      "Loss: 0.013656\n",
      "Loss: 0.013532\n",
      "Loss: 0.013279\n",
      "Loss: 0.013174\n",
      "Loss: 0.012987\n",
      "Loss: 0.012933\n",
      "Loss: 0.012733\n",
      "Loss: 0.012734\n",
      "Loss: 0.012549\n",
      "Loss: 0.012603\n",
      "Loss: 0.012692\n",
      "Loss: 0.012981\n",
      "Loss: 0.012977\n",
      "Loss: 0.012965\n",
      "Loss: 0.012985\n",
      "Loss: 0.012916\n",
      "Loss: 0.012827\n",
      "Loss: 0.012752\n",
      "Loss: 0.012688\n",
      "Loss: 0.012543\n",
      "Loss: 0.012512\n",
      "Loss: 0.012359\n",
      "Loss: 0.012360\n",
      "Loss: 0.012263\n",
      "Loss: 0.012259\n",
      "Loss: 0.012354\n",
      "Loss: 0.012445\n",
      "Loss: 0.012390\n",
      "Loss: 0.012452\n",
      "Loss: 0.012426\n",
      "Loss: 0.012407\n",
      "Loss: 0.012313\n",
      "Loss: 0.012263\n",
      "Loss: 0.012173\n",
      "Loss: 0.012117\n",
      "Loss: 0.012033\n",
      "Loss: 0.011986\n",
      "Loss: 0.011939\n",
      "Loss: 0.011873\n",
      "Loss: 0.011901\n",
      "Loss: 0.011958\n",
      "Loss: 0.011963\n",
      "Loss: 0.011978\n",
      "Loss: 0.011989\n",
      "Loss: 0.011948\n",
      "Loss: 0.011920\n",
      "Loss: 0.011836\n",
      "Loss: 0.011766\n",
      "Loss: 0.011720\n",
      "Loss: 0.011680\n",
      "Loss: 0.011624\n",
      "Loss: 0.011611\n",
      "Loss: 0.011519\n",
      "Loss: 0.011533\n",
      "Loss: 0.011540\n",
      "Loss: 0.011587\n",
      "Loss: 0.011571\n",
      "Loss: 0.011568\n",
      "Loss: 0.011542\n",
      "Loss: 0.011534\n",
      "Loss: 0.011480\n",
      "Loss: 0.011452\n",
      "Loss: 0.011408\n",
      "Loss: 0.011342\n",
      "Loss: 0.011304\n",
      "Loss: 0.011259\n",
      "% Correct on test set: 99.150000\n",
      "Loss: 0.010825\n",
      "Loss: 0.010134\n",
      "Loss: 0.010440\n",
      "Loss: 0.010302\n",
      "Loss: 0.009527\n",
      "Loss: 0.009410\n",
      "Loss: 0.009173\n",
      "Loss: 0.008719\n",
      "Loss: 0.008645\n",
      "Loss: 0.008463\n",
      "Loss: 0.008517\n",
      "Loss: 0.008426\n",
      "Loss: 0.008582\n",
      "Loss: 0.008950\n",
      "Loss: 0.009098\n",
      "Loss: 0.009230\n",
      "Loss: 0.009253\n",
      "Loss: 0.009281\n",
      "Loss: 0.009197\n",
      "Loss: 0.009008\n",
      "Loss: 0.008977\n",
      "Loss: 0.008857\n",
      "Loss: 0.008838\n",
      "Loss: 0.008721\n",
      "Loss: 0.008750\n",
      "Loss: 0.008618\n",
      "Loss: 0.008654\n",
      "Loss: 0.008750\n",
      "Loss: 0.008900\n",
      "Loss: 0.008911\n",
      "Loss: 0.008915\n",
      "Loss: 0.008933\n",
      "Loss: 0.008884\n",
      "Loss: 0.008818\n",
      "Loss: 0.008776\n",
      "Loss: 0.008752\n",
      "Loss: 0.008658\n",
      "Loss: 0.008652\n",
      "Loss: 0.008547\n",
      "Loss: 0.008584\n",
      "Loss: 0.008510\n",
      "Loss: 0.008515\n",
      "Loss: 0.008605\n",
      "Loss: 0.008660\n",
      "Loss: 0.008625\n",
      "Loss: 0.008684\n",
      "Loss: 0.008660\n",
      "Loss: 0.008654\n",
      "Loss: 0.008589\n",
      "Loss: 0.008571\n",
      "Loss: 0.008511\n",
      "Loss: 0.008474\n",
      "Loss: 0.008428\n",
      "Loss: 0.008406\n",
      "Loss: 0.008391\n",
      "Loss: 0.008342\n",
      "Loss: 0.008369\n",
      "Loss: 0.008415\n",
      "Loss: 0.008432\n",
      "Loss: 0.008459\n",
      "Loss: 0.008471\n",
      "Loss: 0.008441\n",
      "Loss: 0.008425\n",
      "Loss: 0.008371\n",
      "Loss: 0.008329\n",
      "Loss: 0.008298\n",
      "Loss: 0.008278\n",
      "Loss: 0.008245\n",
      "Loss: 0.008253\n",
      "Loss: 0.008188\n",
      "Loss: 0.008203\n",
      "Loss: 0.008210\n",
      "Loss: 0.008253\n",
      "Loss: 0.008249\n",
      "Loss: 0.008257\n",
      "Loss: 0.008238\n",
      "Loss: 0.008239\n",
      "Loss: 0.008198\n",
      "Loss: 0.008189\n",
      "Loss: 0.008160\n",
      "Loss: 0.008117\n",
      "Loss: 0.008097\n",
      "Loss: 0.008068\n",
      "% Correct on test set: 99.450000\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    model = train(model, train_data, train_target, iterations=1)\n",
    "    print('%% Correct on test set: %f' % (test(model, test_data, test_target) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dimensions=1)\n",
    "model.weights.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = (train_data[0:5000], train_target[0:5000])\n",
    "alice = (train_data[5000:10000], train_target[5000:10000])\n",
    "sue = (train_data[10000:], train_target[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training round 0\n",
      "\tStep 1: send the model to Bob\n",
      "Loss: 0.250000\n",
      "Loss: 0.219082\n",
      "Loss: 0.293608\n",
      "Loss: 0.259571\n",
      "Loss: 0.223674\n",
      "Loss: 0.204721\n",
      "Loss: 0.182112\n",
      "Loss: 0.163808\n",
      "Loss: 0.150422\n",
      "Loss: 0.140804\n",
      "\tStep 2: send the model to Alice\n",
      "Loss: 0.250000\n",
      "Loss: 0.172907\n",
      "Loss: 0.147506\n",
      "Loss: 0.180480\n",
      "Loss: 0.197209\n",
      "Loss: 0.179812\n",
      "Loss: 0.162350\n",
      "Loss: 0.149679\n",
      "Loss: 0.137586\n",
      "Loss: 0.128769\n",
      "\tStep 3: send the model to Sue\n",
      "Loss: 0.250000\n",
      "Loss: 0.201089\n",
      "Loss: 0.261282\n",
      "Loss: 0.233241\n",
      "Loss: 0.203720\n",
      "Loss: 0.181804\n",
      "Loss: 0.163969\n",
      "Loss: 0.148835\n",
      "Loss: 0.141523\n",
      "Loss: 0.136314\n",
      "Loss: 0.132527\n",
      "Loss: 0.125221\n",
      "Loss: 0.118252\n",
      "Loss: 0.112073\n",
      "Loss: 0.106696\n",
      "Loss: 0.101579\n",
      "Loss: 0.096970\n",
      "Loss: 0.093086\n",
      "Loss: 0.089641\n",
      "Loss: 0.086555\n",
      "Loss: 0.084155\n",
      "Loss: 0.081847\n",
      "Loss: 0.080122\n",
      "Loss: 0.078493\n",
      "Loss: 0.076690\n",
      "Loss: 0.075199\n",
      "Loss: 0.073372\n",
      "Loss: 0.071622\n",
      "Loss: 0.069822\n",
      "Loss: 0.068120\n",
      "Loss: 0.066450\n",
      "Loss: 0.065001\n",
      "Loss: 0.063491\n",
      "Loss: 0.062270\n",
      "Loss: 0.061120\n",
      "Loss: 0.059990\n",
      "Loss: 0.058993\n",
      "Loss: 0.058298\n",
      "Loss: 0.057466\n",
      "Loss: 0.056703\n",
      "Loss: 0.055927\n",
      "Loss: 0.055105\n",
      "Loss: 0.054297\n",
      "Loss: 0.053370\n",
      "Loss: 0.052479\n",
      "Loss: 0.051654\n",
      "Loss: 0.050912\n",
      "Loss: 0.050156\n",
      "Loss: 0.049510\n",
      "Loss: 0.048786\n",
      "Loss: 0.048264\n",
      "Loss: 0.047695\n",
      "Loss: 0.047398\n",
      "Loss: 0.046950\n",
      "Loss: 0.046605\n",
      "Loss: 0.046119\n",
      "Loss: 0.045661\n",
      "Loss: 0.045123\n",
      "Loss: 0.044595\n",
      "Loss: 0.044048\n",
      "Loss: 0.043508\n",
      "Loss: 0.043045\n",
      "Loss: 0.042577\n",
      "\tAverage everyone's new models\n",
      "\t% Correct on test set: 97.800000\n",
      "Starting training round 1\n",
      "\tStep 1: send the model to Bob\n",
      "Loss: 0.027457\n",
      "Loss: 0.028837\n",
      "Loss: 0.030341\n",
      "Loss: 0.029591\n",
      "Loss: 0.027359\n",
      "Loss: 0.026931\n",
      "Loss: 0.025832\n",
      "Loss: 0.024873\n",
      "Loss: 0.024379\n",
      "Loss: 0.024350\n",
      "\tStep 2: send the model to Alice\n",
      "Loss: 0.026210\n",
      "Loss: 0.027375\n",
      "Loss: 0.026414\n",
      "Loss: 0.028860\n",
      "Loss: 0.030706\n",
      "Loss: 0.030969\n",
      "Loss: 0.030360\n",
      "Loss: 0.030052\n",
      "Loss: 0.028944\n",
      "Loss: 0.028013\n",
      "\tStep 3: send the model to Sue\n",
      "Loss: 0.025815\n",
      "Loss: 0.023658\n",
      "Loss: 0.023604\n",
      "Loss: 0.022346\n",
      "Loss: 0.023775\n",
      "Loss: 0.024196\n",
      "Loss: 0.024792\n",
      "Loss: 0.024682\n",
      "Loss: 0.026990\n",
      "Loss: 0.027073\n",
      "Loss: 0.027626\n",
      "Loss: 0.027465\n",
      "Loss: 0.027073\n",
      "Loss: 0.026568\n",
      "Loss: 0.026104\n",
      "Loss: 0.025482\n",
      "Loss: 0.024841\n",
      "Loss: 0.024514\n",
      "Loss: 0.024026\n",
      "Loss: 0.023805\n",
      "Loss: 0.023589\n",
      "Loss: 0.023445\n",
      "Loss: 0.023451\n",
      "Loss: 0.023837\n",
      "Loss: 0.023807\n",
      "Loss: 0.023992\n",
      "Loss: 0.023876\n",
      "Loss: 0.023698\n",
      "Loss: 0.023374\n",
      "Loss: 0.023106\n",
      "Loss: 0.022752\n",
      "Loss: 0.022535\n",
      "Loss: 0.022228\n",
      "Loss: 0.022034\n",
      "Loss: 0.021810\n",
      "Loss: 0.021617\n",
      "Loss: 0.021543\n",
      "Loss: 0.021635\n",
      "Loss: 0.021611\n",
      "Loss: 0.021587\n",
      "Loss: 0.021549\n",
      "Loss: 0.021434\n",
      "Loss: 0.021318\n",
      "Loss: 0.021081\n",
      "Loss: 0.020861\n",
      "Loss: 0.020683\n",
      "Loss: 0.020537\n",
      "Loss: 0.020355\n",
      "Loss: 0.020246\n",
      "Loss: 0.020023\n",
      "Loss: 0.019992\n",
      "Loss: 0.019926\n",
      "Loss: 0.020020\n",
      "Loss: 0.019974\n",
      "Loss: 0.019956\n",
      "Loss: 0.019878\n",
      "Loss: 0.019828\n",
      "Loss: 0.019698\n",
      "Loss: 0.019575\n",
      "Loss: 0.019427\n",
      "Loss: 0.019261\n",
      "Loss: 0.019147\n",
      "Loss: 0.019022\n",
      "\tAverage everyone's new models\n",
      "\t% Correct on test set: 98.300000\n",
      "Starting training round 2\n",
      "\tStep 1: send the model to Bob\n",
      "Loss: 0.019135\n",
      "Loss: 0.019473\n",
      "Loss: 0.020466\n",
      "Loss: 0.020374\n",
      "Loss: 0.018954\n",
      "Loss: 0.018432\n",
      "Loss: 0.017788\n",
      "Loss: 0.017135\n",
      "Loss: 0.016881\n",
      "Loss: 0.016727\n",
      "\tStep 2: send the model to Alice\n",
      "Loss: 0.016163\n",
      "Loss: 0.017154\n",
      "Loss: 0.017653\n",
      "Loss: 0.019679\n",
      "Loss: 0.020630\n",
      "Loss: 0.020934\n",
      "Loss: 0.020742\n",
      "Loss: 0.020735\n",
      "Loss: 0.020162\n",
      "Loss: 0.019440\n",
      "\tStep 3: send the model to Sue\n",
      "Loss: 0.016510\n",
      "Loss: 0.014994\n",
      "Loss: 0.015667\n",
      "Loss: 0.014883\n",
      "Loss: 0.015745\n",
      "Loss: 0.015511\n",
      "Loss: 0.016253\n",
      "Loss: 0.016620\n",
      "Loss: 0.018417\n",
      "Loss: 0.018584\n",
      "Loss: 0.018808\n",
      "Loss: 0.018864\n",
      "Loss: 0.018711\n",
      "Loss: 0.018455\n",
      "Loss: 0.018185\n",
      "Loss: 0.017880\n",
      "Loss: 0.017469\n",
      "Loss: 0.017349\n",
      "Loss: 0.016988\n",
      "Loss: 0.016931\n",
      "Loss: 0.016765\n",
      "Loss: 0.016746\n",
      "Loss: 0.016886\n",
      "Loss: 0.017168\n",
      "Loss: 0.017139\n",
      "Loss: 0.017309\n",
      "Loss: 0.017268\n",
      "Loss: 0.017216\n",
      "Loss: 0.017009\n",
      "Loss: 0.016869\n",
      "Loss: 0.016648\n",
      "Loss: 0.016526\n",
      "Loss: 0.016335\n",
      "Loss: 0.016225\n",
      "Loss: 0.016103\n",
      "Loss: 0.015984\n",
      "Loss: 0.015996\n",
      "Loss: 0.016098\n",
      "Loss: 0.016099\n",
      "Loss: 0.016118\n",
      "Loss: 0.016134\n",
      "Loss: 0.016075\n",
      "Loss: 0.016020\n",
      "Loss: 0.015862\n",
      "Loss: 0.015722\n",
      "Loss: 0.015613\n",
      "Loss: 0.015530\n",
      "Loss: 0.015416\n",
      "Loss: 0.015370\n",
      "Loss: 0.015210\n",
      "Loss: 0.015222\n",
      "Loss: 0.015209\n",
      "Loss: 0.015286\n",
      "Loss: 0.015260\n",
      "Loss: 0.015254\n",
      "Loss: 0.015215\n",
      "Loss: 0.015201\n",
      "Loss: 0.015114\n",
      "Loss: 0.015047\n",
      "Loss: 0.014954\n",
      "Loss: 0.014839\n",
      "Loss: 0.014768\n",
      "Loss: 0.014687\n",
      "\tAverage everyone's new models\n",
      "\t% Correct on test set: 98.800000\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Starting training round %d\" % i)\n",
    "    print(\"\\tStep 1: send the model to Bob\")\n",
    "    bob_model = train(copy.deepcopy(model), bob[0], bob[1], iterations=1)\n",
    "    \n",
    "    print(\"\\tStep 2: send the model to Alice\")\n",
    "    alice_model = train(copy.deepcopy(model), alice[0], alice[1], iterations=1)\n",
    "    \n",
    "    print(\"\\tStep 3: send the model to Sue\")\n",
    "    sue_model = train(copy.deepcopy(model), sue[0], sue[1], iterations=1)\n",
    "    \n",
    "    print(\"\\tAverage everyone's new models\")\n",
    "    model.weights.data = (bob_model.weights.data + alice_model.weights.data + sue_model.weights.data) / 3\n",
    "    \n",
    "    print(\"\\t%% Correct on test set: %f\" % (test(model, test_data, test_target) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacking into federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = [\"my\", \"computer\", \"password\", \"is\", \"pizza\"]\n",
    "victim_input = np.array([[word2index[word] for word in email]])\n",
    "victim_target = np.array([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.250000\n"
     ]
    }
   ],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dimensions=1)\n",
    "model.weights.data *= 0\n",
    "victim_model = train(copy.deepcopy(model), victim_input, victim_target, iterations=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT computer\n",
      "GOT password\n",
      "GOT pizza\n",
      "GOT my\n",
      "GOT is\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(victim_model.weights.data - model.weights.data):\n",
    "    if v != 0:\n",
    "        print(\"GOT\", vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homomorphic encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_key, private_key = phe.generate_paillier_keypair(n_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted values: 383425377072847064575680323960544409719690850472195748727382305161425579523814563448991757841401260406011484778223982626063797971719962251730655074401018251226887264406861243586378480749458619893832526067491876663439673342534567203460037613231429209118328969254772474492037187078776515290140279382933476833986646228157842116083907955289716548794842232054065637840864891900023844675484314697047534567075581502458893657260673588728362429460277157222201780210304752818985266920378854635207319315099306297703675474068281342192896929514656794057182568934603840941411744527575870896306871676307427084500270756317954418710 3300490228008953480684824025290735293115270498229676194639051836761773959582660730577436240442270026277647845147991713632549563644728250390722220004740378967978121189657760013919746963908286202112124015092051419533907997111987765064001210377514965551514590055416635725322257197550673261313953894145555299416204600803207901960015342977904197140142225819315154472354945980828025769765502448212168417376079522454130518532187531564095953199141118772483876735057635752394998505525035421186225071612956738051483321795109114722688445986097722435544473022654531264817810180156022378333131290669748261343417737109285735135522\n",
      "Answer: 8\n"
     ]
    }
   ],
   "source": [
    "x = public_key.encrypt(5)\n",
    "y = public_key.encrypt(3)\n",
    "z = x + y\n",
    "z_ = private_key.decrypt(z)\n",
    "\n",
    "print(\"Encrypted values:\", x.ciphertext(), y.ciphertext())\n",
    "print(\"Answer:\", z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homomorphically encrypted federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Embedding(vocab_size=len(vocab), dimensions=1)\n",
    "model.weights.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_key, private_key = phe.generate_paillier_keypair(n_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_encrypt(model, inp, target, pubkey):\n",
    "    new_model = train(model, inp, target, iterations=1)\n",
    "    \n",
    "    encrypted_weights = np.array([\n",
    "        pubkey.encrypt(v) for v in new_model.weights.data[:,0]\n",
    "    ]).reshape(new_model.weights.data.shape)\n",
    "    return encrypted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training round\n",
      "\tStep 1: Send the model to Bob\n",
      "Loss: 0.250000\n",
      "Loss: 0.219082\n",
      "Loss: 0.293608\n",
      "Loss: 0.259571\n",
      "Loss: 0.223674\n",
      "Loss: 0.204721\n",
      "Loss: 0.182112\n",
      "Loss: 0.163808\n",
      "Loss: 0.150422\n",
      "Loss: 0.140804\n",
      "\tStep 2: Send the model to Alice\n",
      "Loss: 0.250000\n",
      "Loss: 0.172907\n",
      "Loss: 0.147506\n",
      "Loss: 0.180480\n",
      "Loss: 0.197209\n",
      "Loss: 0.179812\n",
      "Loss: 0.162350\n",
      "Loss: 0.149679\n",
      "Loss: 0.137586\n",
      "Loss: 0.128769\n",
      "\tStep 3: Send the model to Sue\n",
      "Loss: 0.250000\n",
      "Loss: 0.201089\n",
      "Loss: 0.261282\n",
      "Loss: 0.233241\n",
      "Loss: 0.203720\n",
      "Loss: 0.181804\n",
      "Loss: 0.163969\n",
      "Loss: 0.148835\n",
      "Loss: 0.141523\n",
      "Loss: 0.136314\n",
      "Loss: 0.132527\n",
      "Loss: 0.125221\n",
      "Loss: 0.118252\n",
      "Loss: 0.112073\n",
      "Loss: 0.106696\n",
      "Loss: 0.101579\n",
      "Loss: 0.096970\n",
      "Loss: 0.093086\n",
      "Loss: 0.089641\n",
      "Loss: 0.086555\n",
      "Loss: 0.084155\n",
      "Loss: 0.081847\n",
      "Loss: 0.080122\n",
      "Loss: 0.078493\n",
      "Loss: 0.076690\n",
      "Loss: 0.075199\n",
      "Loss: 0.073372\n",
      "Loss: 0.071622\n",
      "Loss: 0.069822\n",
      "Loss: 0.068120\n",
      "Loss: 0.066450\n",
      "Loss: 0.065001\n",
      "Loss: 0.063491\n",
      "Loss: 0.062270\n",
      "Loss: 0.061120\n",
      "Loss: 0.059990\n",
      "Loss: 0.058993\n",
      "Loss: 0.058298\n",
      "Loss: 0.057466\n",
      "Loss: 0.056703\n",
      "Loss: 0.055927\n",
      "Loss: 0.055105\n",
      "Loss: 0.054297\n",
      "Loss: 0.053370\n",
      "Loss: 0.052479\n",
      "Loss: 0.051654\n",
      "Loss: 0.050912\n",
      "Loss: 0.050156\n",
      "Loss: 0.049510\n",
      "Loss: 0.048786\n",
      "Loss: 0.048264\n",
      "Loss: 0.047695\n",
      "Loss: 0.047398\n",
      "Loss: 0.046950\n",
      "Loss: 0.046605\n",
      "Loss: 0.046119\n",
      "Loss: 0.045661\n",
      "Loss: 0.045123\n",
      "Loss: 0.044595\n",
      "Loss: 0.044048\n",
      "Loss: 0.043508\n",
      "Loss: 0.043045\n",
      "Loss: 0.042577\n",
      "\tStep 4: Bob, Alice, and Sue send their encrypted models to each other\n",
      "\tStep 5: Only the aggregated model is sent back to the model owner who can decrypt it\n",
      "\t97.800000% correct on test set\n",
      "\n",
      "Starting training round\n",
      "\tStep 1: Send the model to Bob\n",
      "Loss: 0.017486\n",
      "Loss: 0.020543\n",
      "Loss: 0.020148\n",
      "Loss: 0.018263\n",
      "Loss: 0.015889\n",
      "Loss: 0.015823\n",
      "Loss: 0.015073\n",
      "Loss: 0.014585\n",
      "Loss: 0.013838\n",
      "Loss: 0.013545\n",
      "\tStep 2: Send the model to Alice\n",
      "Loss: 0.016120\n",
      "Loss: 0.013936\n",
      "Loss: 0.014703\n",
      "Loss: 0.017603\n",
      "Loss: 0.019066\n",
      "Loss: 0.019812\n",
      "Loss: 0.018990\n",
      "Loss: 0.018772\n",
      "Loss: 0.018045\n",
      "Loss: 0.017301\n",
      "\tStep 3: Send the model to Sue\n",
      "Loss: 0.010848\n",
      "Loss: 0.009498\n",
      "Loss: 0.010747\n",
      "Loss: 0.009393\n",
      "Loss: 0.011467\n",
      "Loss: 0.012196\n",
      "Loss: 0.012237\n",
      "Loss: 0.012694\n",
      "Loss: 0.015105\n",
      "Loss: 0.014783\n",
      "Loss: 0.015676\n",
      "Loss: 0.015773\n",
      "Loss: 0.015704\n",
      "Loss: 0.015172\n",
      "Loss: 0.014962\n",
      "Loss: 0.014462\n",
      "Loss: 0.014046\n",
      "Loss: 0.013963\n",
      "Loss: 0.013726\n",
      "Loss: 0.013500\n",
      "Loss: 0.013388\n",
      "Loss: 0.013281\n",
      "Loss: 0.013422\n",
      "Loss: 0.014218\n",
      "Loss: 0.014250\n",
      "Loss: 0.014351\n",
      "Loss: 0.014267\n",
      "Loss: 0.014100\n",
      "Loss: 0.013851\n",
      "Loss: 0.013816\n",
      "Loss: 0.013597\n",
      "Loss: 0.013471\n",
      "Loss: 0.013225\n",
      "Loss: 0.013111\n",
      "Loss: 0.012937\n",
      "Loss: 0.012818\n",
      "Loss: 0.012920\n",
      "Loss: 0.013162\n",
      "Loss: 0.013322\n",
      "Loss: 0.013375\n",
      "Loss: 0.013354\n",
      "Loss: 0.013336\n",
      "Loss: 0.013292\n",
      "Loss: 0.013163\n",
      "Loss: 0.012984\n",
      "Loss: 0.012877\n",
      "Loss: 0.012802\n",
      "Loss: 0.012677\n",
      "Loss: 0.012629\n",
      "Loss: 0.012466\n",
      "Loss: 0.012442\n",
      "Loss: 0.012473\n",
      "Loss: 0.012781\n",
      "Loss: 0.012716\n",
      "Loss: 0.012753\n",
      "Loss: 0.012706\n",
      "Loss: 0.012673\n",
      "Loss: 0.012578\n",
      "Loss: 0.012511\n",
      "Loss: 0.012399\n",
      "Loss: 0.012300\n",
      "Loss: 0.012250\n",
      "Loss: 0.012158\n",
      "\tStep 4: Bob, Alice, and Sue send their encrypted models to each other\n",
      "\tStep 5: Only the aggregated model is sent back to the model owner who can decrypt it\n",
      "\t98.300000% correct on test set\n",
      "\n",
      "Starting training round\n",
      "\tStep 1: Send the model to Bob\n",
      "Loss: 0.014086\n",
      "Loss: 0.017484\n",
      "Loss: 0.015607\n",
      "Loss: 0.014389\n",
      "Loss: 0.012477\n",
      "Loss: 0.011958\n",
      "Loss: 0.011619\n",
      "Loss: 0.011434\n",
      "Loss: 0.010787\n",
      "Loss: 0.010255\n",
      "\tStep 2: Send the model to Alice\n",
      "Loss: 0.010525\n",
      "Loss: 0.007101\n",
      "Loss: 0.010146\n",
      "Loss: 0.013853\n",
      "Loss: 0.015516\n",
      "Loss: 0.016342\n",
      "Loss: 0.015116\n",
      "Loss: 0.014709\n",
      "Loss: 0.014399\n",
      "Loss: 0.013731\n",
      "\tStep 3: Send the model to Sue\n",
      "Loss: 0.007708\n",
      "Loss: 0.007029\n",
      "Loss: 0.007832\n",
      "Loss: 0.006578\n",
      "Loss: 0.008438\n",
      "Loss: 0.008409\n",
      "Loss: 0.008451\n",
      "Loss: 0.009342\n",
      "Loss: 0.011705\n",
      "Loss: 0.011617\n",
      "Loss: 0.012623\n",
      "Loss: 0.012493\n",
      "Loss: 0.012628\n",
      "Loss: 0.012129\n",
      "Loss: 0.011967\n",
      "Loss: 0.011617\n",
      "Loss: 0.011203\n",
      "Loss: 0.011200\n",
      "Loss: 0.011035\n",
      "Loss: 0.010840\n",
      "Loss: 0.010571\n",
      "Loss: 0.010511\n",
      "Loss: 0.010729\n",
      "Loss: 0.011631\n",
      "Loss: 0.011741\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"\\nStarting training round\")\n",
    "    print(\"\\tStep 1: Send the model to Bob\")\n",
    "    bob_encrypted_model = train_and_encrypt(copy.deepcopy(model), bob[0], bob[1], public_key)\n",
    "    \n",
    "    print(\"\\tStep 2: Send the model to Alice\")\n",
    "    alice_encrypted_model = train_and_encrypt(copy.deepcopy(model), alice[0], alice[1], public_key)\n",
    "    \n",
    "    print(\"\\tStep 3: Send the model to Sue\")\n",
    "    sue_encrypted_model = train_and_encrypt(copy.deepcopy(model), sue[0], sue[1], public_key)\n",
    "    \n",
    "    print(\"\\tStep 4: Bob, Alice, and Sue send their encrypted models to each other\")\n",
    "    aggregated_model = bob_encrypted_model + alice_encrypted_model + sue_encrypted_model\n",
    "    \n",
    "    print(\"\\tStep 5: Only the aggregated model is sent back to the model owner who can decrypt it\")\n",
    "    raw_values = [private_key.decrypt(v) for v in aggregated_model.flatten()]\n",
    "    model.weights.data = np.array(raw_values).reshape(model.weights.data.shape)\n",
    "    \n",
    "    print(\"\\t%f%% correct on test set\" % (test(model, test_data, test_target) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
