{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use definitions from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        self.id = np.random.randint(0, 100000) if id is None else id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if (self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for _, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if not self.autograd:\n",
    "            return\n",
    "        \n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self.data))\n",
    "        \n",
    "        if grad_origin is not None:\n",
    "            if self.children[grad_origin.id] == 0:\n",
    "                return\n",
    "                raise Exception(\"cannot backprop more than once\")\n",
    "\n",
    "            self.children[grad_origin.id] -= 1\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "        \n",
    "        assert grad.autograd == False\n",
    "        \n",
    "        if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):\n",
    "            if self.creation_op == \"neg\":\n",
    "                self.creators[0].backward(self.grad.__neg__(), self)\n",
    "            elif self.creation_op == \"add\":\n",
    "                self.creators[0].backward(self.grad, self)\n",
    "                self.creators[1].backward(self.grad, self)\n",
    "            elif self.creation_op == \"sub\":\n",
    "                positive_grad = Tensor(self.grad.data)\n",
    "                self.creators[0].backward(positive_grad, self)\n",
    "                negative_grad = Tensor(self.grad.__neg__().data)\n",
    "                self.creators[1].backward(negative_grad, self)\n",
    "            elif self.creation_op == \"mul\":\n",
    "                mul_grad_0 = self.grad * self.creators[0]\n",
    "                self.creators[0].backward(mul_grad_0, self)\n",
    "                mul_grad_1 = self.grad * self.creators[1]\n",
    "                self.creators[1].backward(mul_grad_1, self)\n",
    "            elif self.creation_op == \"transpose\":\n",
    "                self.creators[0].backward(self.grad.transpose())\n",
    "            elif self.creation_op == \"mm\":\n",
    "                activations = self.creators[0]\n",
    "                weights = self.creators[1]\n",
    "                activations_grad = self.grad.mm(weights.transpose())\n",
    "                activations.backward(activations_grad)\n",
    "                weights_grad = self.grad.transpose().mm(activations).transpose()\n",
    "                weights.backward(weights_grad)\n",
    "            elif \"sum\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                ds = self.creators[0].data.shape[dim]\n",
    "                self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "            elif \"expand\" in self.creation_op:\n",
    "                dim = int(self.creation_op.split(\"_\")[1])\n",
    "                self.creators[0].backward(self.grad.sum(dim))\n",
    "            elif self.creation_op == \"sigmoid\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * self * (ones - self))\n",
    "            elif self.creation_op == \"tanh\":\n",
    "                ones = Tensor(np.ones_like(self.grad.data))\n",
    "                self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "            elif self.creation_op == \"index_select\":\n",
    "                new_grad = np.zeros_like(self.creators[0].data)\n",
    "                indices = self.index_select_indices.data.flatten()\n",
    "                grad_reshaped = grad.data.reshape(len(indices), -1)\n",
    "                for i in range(len(indices)):\n",
    "                    new_grad[indices[i]] += grad_reshaped[i]\n",
    "                self.creators[0].backward(Tensor(new_grad))\n",
    "            elif self.creation_op == \"cross_entropy\":\n",
    "                dx = self.softmax_output - self.target_dist\n",
    "                self.creators[0].backward(Tensor(dx))\n",
    "            \n",
    "\n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self, other], creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self, other], creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def sum(self, dimension):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dimension), autograd=True, creators=[self], creation_op=\"sum_\"+str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    def expand(self, dimension, copies):\n",
    "        transpose_cmd = list(range(0, len(self.data.shape)))\n",
    "        transpose_cmd.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape).transpose(transpose_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dimension))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd and x.autograd:\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self, x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            tensor = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op=\"index_select\")\n",
    "            tensor.index_select_indices = indices\n",
    "            return tensor\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        return softmax_output\n",
    "\n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape) - 1, keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t), -1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * target_dist).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            tensor = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
    "            tensor.softmax_output = softmax_output\n",
    "            tensor.target_dist = target_dist\n",
    "            return tensor\n",
    "        return Tensor(loss)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_bias = bias\n",
    "        \n",
    "        weights = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/n_inputs)\n",
    "        self.weights = Tensor(weights, autograd=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.biases = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weights)\n",
    "        if self.use_bias:\n",
    "            self.parameters.append(self.biases)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.use_bias:\n",
    "            return input.mm(self.weights) + self.biases.expand(0, len(input.data))\n",
    "        return input.mm(self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        weights = (np.random.rand(vocab_size, dimensions) - 0.5) / dimensions\n",
    "        self.weights = Tensor(weights, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weights.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation=\"sigmoid\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "        \n",
    "        self.weights_ih = Linear(n_inputs, n_hidden)\n",
    "        self.weights_hh = Linear(n_hidden, n_hidden)\n",
    "        self.weights_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.weights_ih.get_parameters()\n",
    "        self.parameters += self.weights_hh.get_parameters()\n",
    "        self.parameters += self.weights_ho.get_parameters()\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_previous_hidden = self.weights_hh.forward(hidden)\n",
    "        combined = self.weights_ih.forward(input) + from_previous_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.weights_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.grad.data *= 0\n",
    "\n",
    "    def step(self, zero=True):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.data -= parameter.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                parameter.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/shakespeare/shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab consists of characters, not words!\n",
    "vocab = list(set(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dimensions=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=1\n",
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int(len(indices) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches - 1) / bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iter:0\tBatch:1/195\tLoss:93.41064503197084\n",
      "\r",
      "Iter:0\tBatch:2/195\tLoss:7.7272044771709e+75\n",
      "\r",
      "Iter:0\tBatch:3/195\tLoss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:168: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tBatch:4/195\tLoss:nan\n",
      "Iter:0\tBatch:5/195\tLoss:nan\n",
      "Iter:0\tBatch:6/195\tLoss:nan\n",
      "Iter:0\tBatch:7/195\tLoss:nan\n",
      "Iter:0\tBatch:8/195\tLoss:nan\n",
      "Iter:0\tBatch:9/195\tLoss:nan\n",
      "Iter:0\tBatch:10/195\tLoss:nan\n",
      "Iter:0\tBatch:11/195\tLoss:nan\n",
      "Iter:0\tBatch:12/195\tLoss:nan\n",
      "Iter:0\tBatch:13/195\tLoss:nan\n",
      "Iter:0\tBatch:14/195\tLoss:nan\n",
      "Iter:0\tBatch:15/195\tLoss:nan\n",
      "Iter:0\tBatch:16/195\tLoss:nan\n",
      "Iter:0\tBatch:17/195\tLoss:nan\n",
      "Iter:0\tBatch:18/195\tLoss:nan\n",
      "Iter:0\tBatch:19/195\tLoss:nan\n",
      "Iter:0\tBatch:20/195\tLoss:nan\n",
      "Iter:0\tBatch:21/195\tLoss:nan\n",
      "Iter:0\tBatch:22/195\tLoss:nan\n",
      "Iter:0\tBatch:23/195\tLoss:nan\n",
      "Iter:0\tBatch:24/195\tLoss:nan\n",
      "Iter:0\tBatch:25/195\tLoss:nan\n",
      "Iter:0\tBatch:26/195\tLoss:nan\n",
      "Iter:0\tBatch:27/195\tLoss:nan\n",
      "Iter:0\tBatch:28/195\tLoss:nan\n",
      "Iter:0\tBatch:29/195\tLoss:nan\n",
      "Iter:0\tBatch:30/195\tLoss:nan\n",
      "Iter:0\tBatch:31/195\tLoss:nan\n",
      "Iter:0\tBatch:32/195\tLoss:nan\n",
      "Iter:0\tBatch:33/195\tLoss:nan\n",
      "Iter:0\tBatch:34/195\tLoss:nan\n",
      "Iter:0\tBatch:35/195\tLoss:nan\n",
      "Iter:0\tBatch:36/195\tLoss:nan\n",
      "Iter:0\tBatch:37/195\tLoss:nan\n",
      "Iter:0\tBatch:38/195\tLoss:nan\n",
      "Iter:0\tBatch:39/195\tLoss:nan\n",
      "Iter:0\tBatch:40/195\tLoss:nan\n",
      "Iter:0\tBatch:41/195\tLoss:nan\n",
      "Iter:0\tBatch:42/195\tLoss:nan\n",
      "Iter:0\tBatch:43/195\tLoss:nan\n",
      "Iter:0\tBatch:44/195\tLoss:nan\n",
      "Iter:0\tBatch:45/195\tLoss:nan\n",
      "Iter:0\tBatch:46/195\tLoss:nan\n",
      "Iter:0\tBatch:47/195\tLoss:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\tBatch:48/195\tLoss:nan\n",
      "Iter:0\tBatch:49/195\tLoss:nan\n",
      "Iter:0\tBatch:50/195\tLoss:nan\n",
      "Iter:0\tBatch:51/195\tLoss:nan\n",
      "Iter:0\tBatch:52/195\tLoss:nan\n",
      "Iter:0\tBatch:53/195\tLoss:nan\n",
      "Iter:0\tBatch:54/195\tLoss:nan\n",
      "Iter:0\tBatch:55/195\tLoss:nan\n",
      "Iter:0\tBatch:56/195\tLoss:nan\n",
      "Iter:0\tBatch:57/195\tLoss:nan\n",
      "Iter:0\tBatch:58/195\tLoss:nan\n",
      "Iter:0\tBatch:59/195\tLoss:nan\n",
      "Iter:0\tBatch:60/195\tLoss:nan\n",
      "Iter:0\tBatch:61/195\tLoss:nan\n",
      "Iter:0\tBatch:62/195\tLoss:nan\n",
      "Iter:0\tBatch:63/195\tLoss:nan\n",
      "Iter:0\tBatch:64/195\tLoss:nan\n",
      "Iter:0\tBatch:65/195\tLoss:nan\n",
      "Iter:0\tBatch:66/195\tLoss:nan\n",
      "Iter:0\tBatch:67/195\tLoss:nan\n",
      "Iter:0\tBatch:68/195\tLoss:nan\n",
      "Iter:0\tBatch:69/195\tLoss:nan\n",
      "Iter:0\tBatch:70/195\tLoss:nan\n",
      "Iter:0\tBatch:71/195\tLoss:nan\n",
      "Iter:0\tBatch:72/195\tLoss:nan\n",
      "Iter:0\tBatch:73/195\tLoss:nan\n",
      "Iter:0\tBatch:74/195\tLoss:nan\n",
      "Iter:0\tBatch:75/195\tLoss:nan\n",
      "Iter:0\tBatch:76/195\tLoss:nan\n",
      "Iter:0\tBatch:77/195\tLoss:nan\n",
      "Iter:0\tBatch:78/195\tLoss:nan\n",
      "Iter:0\tBatch:79/195\tLoss:nan\n",
      "Iter:0\tBatch:80/195\tLoss:nan\n",
      "Iter:0\tBatch:81/195\tLoss:nan\n",
      "Iter:0\tBatch:82/195\tLoss:nan\n",
      "Iter:0\tBatch:83/195\tLoss:nan\n",
      "Iter:0\tBatch:84/195\tLoss:nan\n",
      "Iter:0\tBatch:85/195\tLoss:nan\n",
      "Iter:0\tBatch:86/195\tLoss:nan\n",
      "Iter:0\tBatch:87/195\tLoss:nan\n",
      "Iter:0\tBatch:88/195\tLoss:nan\n",
      "Iter:0\tBatch:89/195\tLoss:nan\n",
      "Iter:0\tBatch:90/195\tLoss:nan\n",
      "Iter:0\tBatch:91/195\tLoss:nan\n",
      "Iter:0\tBatch:92/195\tLoss:nan\n",
      "Iter:0\tBatch:93/195\tLoss:nan\n",
      "Iter:0\tBatch:94/195\tLoss:nan\n",
      "Iter:0\tBatch:95/195\tLoss:nan\n",
      "Iter:0\tBatch:96/195\tLoss:nan\n",
      "Iter:0\tBatch:97/195\tLoss:nan\n",
      "Iter:0\tBatch:98/195\tLoss:nan\n",
      "Iter:0\tBatch:99/195\tLoss:nan\n",
      "Iter:0\tBatch:100/195\tLoss:nan\n",
      "Iter:0\tBatch:101/195\tLoss:nan\n",
      "Iter:0\tBatch:102/195\tLoss:nan\n",
      "Iter:0\tBatch:103/195\tLoss:nan\n",
      "Iter:0\tBatch:104/195\tLoss:nan\n",
      "Iter:0\tBatch:105/195\tLoss:nan\n",
      "Iter:0\tBatch:106/195\tLoss:nan\n",
      "Iter:0\tBatch:107/195\tLoss:nan\n",
      "Iter:0\tBatch:108/195\tLoss:nan\n",
      "Iter:0\tBatch:109/195\tLoss:nan\n",
      "Iter:0\tBatch:110/195\tLoss:nan\n",
      "Iter:0\tBatch:111/195\tLoss:nan\n",
      "Iter:0\tBatch:112/195\tLoss:nan\n",
      "Iter:0\tBatch:113/195\tLoss:nan\n",
      "Iter:0\tBatch:114/195\tLoss:nan\n",
      "Iter:0\tBatch:115/195\tLoss:nan\n",
      "Iter:0\tBatch:116/195\tLoss:nan\n",
      "Iter:0\tBatch:117/195\tLoss:nan\n",
      "Iter:0\tBatch:118/195\tLoss:nan\n",
      "Iter:0\tBatch:119/195\tLoss:nan\n",
      "Iter:0\tBatch:120/195\tLoss:nan\n",
      "Iter:0\tBatch:121/195\tLoss:nan\n",
      "Iter:0\tBatch:122/195\tLoss:nan\n",
      "Iter:0\tBatch:123/195\tLoss:nan\n",
      "Iter:0\tBatch:124/195\tLoss:nan\n",
      "Iter:0\tBatch:125/195\tLoss:nan\n",
      "Iter:0\tBatch:126/195\tLoss:nan\n",
      "Iter:0\tBatch:127/195\tLoss:nan\n",
      "Iter:0\tBatch:128/195\tLoss:nan\n",
      "Iter:0\tBatch:129/195\tLoss:nan\n",
      "Iter:0\tBatch:130/195\tLoss:nan\n",
      "Iter:0\tBatch:131/195\tLoss:nan\n",
      "Iter:0\tBatch:132/195\tLoss:nan\n",
      "Iter:0\tBatch:133/195\tLoss:nan\n",
      "Iter:0\tBatch:134/195\tLoss:nan\n",
      "Iter:0\tBatch:135/195\tLoss:nan\n",
      "Iter:0\tBatch:136/195\tLoss:nan\n",
      "Iter:0\tBatch:137/195\tLoss:nan\n",
      "Iter:0\tBatch:138/195\tLoss:nan\n",
      "Iter:0\tBatch:139/195\tLoss:nan\n",
      "Iter:0\tBatch:140/195\tLoss:nan\n",
      "Iter:0\tBatch:141/195\tLoss:nan\n",
      "Iter:0\tBatch:142/195\tLoss:nan\n",
      "Iter:0\tBatch:143/195\tLoss:nan\n",
      "Iter:0\tBatch:144/195\tLoss:nan\n",
      "Iter:0\tBatch:145/195\tLoss:nan\n",
      "Iter:0\tBatch:146/195\tLoss:nan\n",
      "Iter:0\tBatch:147/195\tLoss:nan\n",
      "Iter:0\tBatch:148/195\tLoss:nan\n",
      "Iter:0\tBatch:149/195\tLoss:nan\n",
      "Iter:0\tBatch:150/195\tLoss:nan\n",
      "Iter:0\tBatch:151/195\tLoss:nan\n",
      "Iter:0\tBatch:152/195\tLoss:nan\n",
      "Iter:0\tBatch:153/195\tLoss:nan\n",
      "Iter:0\tBatch:154/195\tLoss:nan\n",
      "Iter:0\tBatch:155/195\tLoss:nan\n",
      "Iter:0\tBatch:156/195\tLoss:nan\n",
      "Iter:0\tBatch:157/195\tLoss:nan\n",
      "Iter:0\tBatch:158/195\tLoss:nan\n",
      "Iter:0\tBatch:159/195\tLoss:nan\n",
      "Iter:0\tBatch:160/195\tLoss:nan\n",
      "Iter:0\tBatch:161/195\tLoss:nan\n",
      "Iter:0\tBatch:162/195\tLoss:nan\n",
      "Iter:0\tBatch:163/195\tLoss:nan\n",
      "Iter:0\tBatch:164/195\tLoss:nan\n",
      "Iter:0\tBatch:165/195\tLoss:nan\n",
      "Iter:0\tBatch:166/195\tLoss:nan\n",
      "Iter:0\tBatch:167/195\tLoss:nan\n",
      "Iter:0\tBatch:168/195\tLoss:nan\n",
      "Iter:0\tBatch:169/195\tLoss:nan\n",
      "Iter:0\tBatch:170/195\tLoss:nan\n",
      "Iter:0\tBatch:171/195\tLoss:nan\n",
      "Iter:0\tBatch:172/195\tLoss:nan\n",
      "Iter:0\tBatch:173/195\tLoss:nan\n",
      "Iter:0\tBatch:174/195\tLoss:nan\n",
      "Iter:0\tBatch:175/195\tLoss:nan\n",
      "Iter:0\tBatch:176/195\tLoss:nan\n",
      "Iter:0\tBatch:177/195\tLoss:nan\n",
      "Iter:0\tBatch:178/195\tLoss:nan\n",
      "Iter:0\tBatch:179/195\tLoss:nan\n",
      "Iter:0\tBatch:180/195\tLoss:nan\n",
      "Iter:0\tBatch:181/195\tLoss:nan\n",
      "Iter:0\tBatch:182/195\tLoss:nan\n",
      "Iter:0\tBatch:183/195\tLoss:nan\n",
      "Iter:0\tBatch:184/195\tLoss:nan\n",
      "Iter:0\tBatch:185/195\tLoss:nan\n",
      "Iter:0\tBatch:186/195\tLoss:nan\n",
      "Iter:0\tBatch:187/195\tLoss:nan\n",
      "Iter:0\tBatch:188/195\tLoss:nan\n",
      "Iter:0\tBatch:189/195\tLoss:nan\n",
      "Iter:0\tBatch:190/195\tLoss:nan\n",
      "Iter:0\tBatch:191/195\tLoss:nan\n",
      "Iter:0\tBatch:192/195\tLoss:nan\n",
      "Iter:0\tBatch:193/195\tLoss:nan\n",
      "Iter:0\tBatch:194/195\tLoss:nan\n",
      "Iter:0\tBatch:195/195\tLoss:nan\n"
     ]
    }
   ],
   "source": [
    "def train(iterations=iterations):\n",
    "    for iteration in range(iterations):\n",
    "        total_loss = 0\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        n_batches = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(n_batches):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            \n",
    "            for t in range(bptt):\n",
    "                embed_input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=embed_input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                loss = batch_loss if t == 0 else loss + batch_loss\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data / bptt\n",
    "            \n",
    "            log = \"\\rIter:\" + str(iteration)\n",
    "            log += \"\\tBatch:\" + str(batch_i + 1) + \"/\" + str(len(input_batches))\n",
    "            log += \"\\tLoss:\" + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            print(log)\n",
    "        \n",
    "        optimizer.alpha *= 0.99\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    embed_input = Tensor(np.array([word2index[init_char]]))\n",
    "\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(embed_input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden = hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        maximum = (temp_dist > np.random.rand()).argmax()\n",
    "        char = vocab[maximum]\n",
    "        s += char\n",
    "        embed_input = Tensor(np.array([maximum]))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/grok/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid activations\n",
      "[0.93940638 0.96852968]\n",
      "[0.9919462  0.99121735]\n",
      "[0.99301385 0.99302901]\n",
      "[0.9930713  0.99307098]\n",
      "[0.99307285 0.99307285]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "\n",
      "Sigmoid gradients\n",
      "[0.03439552 0.03439552]\n",
      "[0.00118305 0.00118305]\n",
      "[4.06916726e-05 4.06916726e-05]\n",
      "[1.39961115e-06 1.39961115e-06]\n",
      "[4.81403643e-08 4.81403637e-08]\n",
      "[1.65582672e-09 1.65582765e-09]\n",
      "[5.69682675e-11 5.69667160e-11]\n",
      "[1.97259346e-12 1.97517920e-12]\n",
      "[8.45387597e-14 8.02306381e-14]\n",
      "[1.45938177e-14 2.16938983e-14]\n",
      "\n",
      "Relu activations\n",
      "[4.8135251  4.72615519]\n",
      "[23.71814585 23.98025559]\n",
      "[119.63916823 118.852839  ]\n",
      "[595.05052421 597.40951192]\n",
      "[2984.68857188 2977.61160877]\n",
      "[14895.13500696 14916.36589628]\n",
      "[74560.59859209 74496.90592414]\n",
      "[372548.22228863 372739.30029248]\n",
      "[1863505.42345854 1862932.18944699]\n",
      "[9315234.18124649 9316953.88328115]\n",
      "\n",
      "Relu gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "relu = lambda x: (x > 0).astype(float) * x\n",
    "\n",
    "weights = np.array([[1,4], [4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "\n",
    "print(\"Sigmoid activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "    \n",
    "print(\"\\nSigmoid gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient).dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print(\"\\nRelu activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "    \n",
    "print(\"\\nRelu gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_output = n_output\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        \n",
    "        self.hf = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_inputs, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_inputs, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "\n",
    "        f = self.xf.forward(input) + self.hf.forward(prev_hidden)\n",
    "        i = self.xi.forward(input) + self.hi.forward(prev_hidden)\n",
    "        o = self.xo.forward(input) + self.ho.forward(prev_hidden)\n",
    "        g = self.xc.forward(input) + self.hc.forward(prev_hidden)\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        h = o * c.tanh()\n",
    "\n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        init_hidden = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        init_cell = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        init_hidden.data[:,0] += 1\n",
    "        init_cell.data[:,0] += 1\n",
    "        return (init_hidden, init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dimensions=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int(len(indices) / batch_size)\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches-1) / bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
